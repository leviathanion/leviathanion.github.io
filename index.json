[{"categories":["编程语言"],"content":"on java 8学习笔记 ","date":"2022-07-19","objectID":"/onjava8/:0:0","tags":["编程语言"],"title":"Onjava8","uri":"/onjava8/"},{"categories":["编程语言"],"content":"前言 Java是一种派生语言，是对C++的改进，核心改变是引入了虚拟机和GC机制 Java中最有影响力的编程思想面向对象概念来源于SmallTalk 要求所有东西(尤其是到最底层)都是对象是一种设计错误 但面向对象思想在某些情况下仍然能够发挥有效的作用 Java的一些其他特性并没有设计的如此成功 明白有些功能特性为什么会被抛弃 了解边界在哪可以更好的设计和编程 编程是管理复杂度的过程。 很多语言由于历史包袱不可避免的导致撞墙，例如C++为了兼容C，VB为了兼容BASIC产生了一些不可维护的语法，Perl为了兼容一些unix工具导致可读性极差等。 但这些语言必然是为了解决某类复杂问题而产生的，因此在某类问题方面非常成功。 ","date":"2022-07-19","objectID":"/onjava8/:1:0","tags":["编程语言"],"title":"Onjava8","uri":"/onjava8/"},{"categories":["编程语言"],"content":"对象 ","date":"2022-07-19","objectID":"/onjava8/:2:0","tags":["编程语言"],"title":"Onjava8","uri":"/onjava8/"},{"categories":["linux"],"content":"会话管理 新建会话tmux new -s \u003csession-name\u003e 列出所有会话tmux ls或者Ctrl+b s 退出会话tmux detach或者Ctrl+b d 接入会话tmux attach -t \u003c编号\u003e/\u003csession-name\u003e 杀死对话tmux kill-session -t \u003c编号\u003e/\u003csession-name\u003e 切换会话tmux switch -t \u003c编号\u003e/\u003csession-name\u003e 重命名会话tmux rename-session -t \u003c编号\u003e/\u003csession-name\u003e或者Ctrl+b $ 窗口管理 创建一个新的窗口Ctrl+b c 列出所有窗口Ctrl+b w 窗格管理 划分左右两个窗格Ctrl+b % 划分上下两个窗格Ctrl+b \" 移动窗格Ctrl+b 方向键 关闭当前窗格Ctrl+b x 调整窗格大小Ctrl+b Ctrl+方向键 ","date":"2022-07-05","objectID":"/tmux%E4%BD%BF%E7%94%A8/:0:0","tags":["linux"],"title":"Tmux使用","uri":"/tmux%E4%BD%BF%E7%94%A8/"},{"categories":["机器学习","深度学习","KG"],"content":"SimKGC模型 ","date":"2022-07-01","objectID":"/simkgc/:0:0","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"Novelty 使用InfoNCE损失改进原有损失 使用IB,PB和SN三种负样本种类来增加负样本数量 使用图结构重排序来感知图结构 ","date":"2022-07-01","objectID":"/simkgc/:1:0","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"问题和动机 最新的基于文本（预训练模型）的方法可以访问到额外的输入信息 使用预训练模型，基于文本的方法性能仍然大幅弱于基于嵌入的方法 可能的原因是基于文本的方法中对比学习的效率过低 基于嵌入的方法不涉及昂贵的文本编码计算，因此可以使用大量负样本 默认配置下RotatE在Wikidata5M数据集上可以训练1000个epoch，负样本数量为64时，基于文本的方法KEPLER只能训练30个epoch，负样本数量为1 因此改进现有基于文本的方法的对比学习损失，使其可以使用更多的负样本有概率提升模型的性能 基于文本的方法可以归纳学习到训练过中看不到的实体的表示 ","date":"2022-07-01","objectID":"/simkgc/:2:0","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"idea 使用的新的架构，引入新的负样本，从而增加负样本的个数 采用Bi-encoder的架构，可以增大Batch，从而增大in-batch negatives的数量 缓存之前的batch，并作为pre-batch negatives，增加负样本数量 (h,r,h)这种类型的三元组同一作为hard negatives可以提高对比学习的性能 将损失从margin-based排序损失更改为InfoNCE损失，可以让模型专注于hard negatives 基于文本的损失关注于语义匹配而忽略了图结构信息，因此根据距离的远近进行简单的重新排序来捕获图结构信息 ","date":"2022-07-01","objectID":"/simkgc/:3:0","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"模型 ","date":"2022-07-01","objectID":"/simkgc/:4:0","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"Bi-encoder架构 两个编码器使用相同的预训练模型，但不共享参数 对每个关系构建一个逆关系，即$(h,r,t) \\rightarrow (t,r^{-1},h)$，因此所有的补全可以转换为尾实体的预测 头实体和关系的嵌入 将h和r的句子描述使用一个[SEP]符号链接起来，输入到$BERT_{hr}$中，将最后一层的隐藏状态使用均值池化，然后进行L2归一化，获得关系感知嵌入$e_{hr}$ 即使头实体相同，但对于不同的关系可以得到不同的表示 尾实体的嵌入与头实体和关系类似，将尾实体的描述输入到$BERT_t$中，进行均值池化和L2归一化，获得尾实体嵌入$e_{t}$ 得分函数为余弦相似度，对于预测任务$(h,r?)$，预测结果为 $$\\arg\\max_{t_i}cos(e_{hr},e_{t_i}),t_i \\in \\mathcal{E}$$ ","date":"2022-07-01","objectID":"/simkgc/:4:1","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"负采样 In-batch Negatives(IB)，同一个Batch中的三元组都属于负样本，这种策略可以复用bi-encoder模型的嵌入 Pre-batch Negatives(PB)，取上n个参数版本的模型同一个batch的嵌入作为负样本，n通常取1或者2（MoCo有更多更新的负样本采集工作，本文没有探究） Self-Negatives(SN)，将$(h,r,h)$作为硬否定来增强模型性能(硬否定参考simcse) 模型中可能存在一种误判，正确实体有可能出现在负样本中，因此过滤掉这些实体。 最终的负样本数量为三者总和，假设batchsize为1024，则负样本个数为1023+nx1024+1 ","date":"2022-07-01","objectID":"/simkgc/:4:2","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"图结构感知 距离当前节点更近的节点更有可能是正确答案，因此添加一个结构感知的值 $$\\arg\\max_{t_i} cos(e_{hr},e_{t_i}) + \\alpha \\mathbb{1}(t_i \\in \\mathcal{E}_k (h)) $$ ","date":"2022-07-01","objectID":"/simkgc/:4:3","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"损失 采用附加margin的InfoNCE损失 \\begin{align*} \\mathcal{L} = -\\log \\frac{e^{(\\phi(h,r,t)-\\gamma)/\\tau}} {e^{(\\phi(h,r,t)-\\gamma)/\\tau} + \\sum_{i=1}^{|\\mathcal{N}|}{e^{\\phi(h,r,t_i^\\prime)/\\tau}}} \\end{align*} 附加margin是为了鼓励模型增加正确三元组的分数 温度$\\tau$可以调整负样本的相对重要性，越小，越专注于硬负样本 ","date":"2022-07-01","objectID":"/simkgc/:4:4","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"实验 使用WN18RR和FB15K-237，以及Wikidata5M作为数据集来训练 Wikidata5M含有五百万个实体和2000万个三元组，提供了两种设置，transductive，测试集中的所有实体也出现在训练集中，inductive测试集和训练集没有重合 实验效果，在Wikidata5M上全面优于别的模型，在WN18RR上全面优于别的模型,在FB15K-237上稍有落后 ","date":"2022-07-01","objectID":"/simkgc/:5:0","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"有效性分析 ","date":"2022-07-01","objectID":"/simkgc/:6:0","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"损失函数与负样本 模型有效可能有两个原因 新的损失函数设计 更多的负样本 改变负样本个数和损失函数，得到如下结果 可以看出二者皆有一定的影响，但损失函数的影响似乎更大，InfoNCE中，硬负样本会产生更大的梯度，添加更多的负样本可以产生更稳健的表示。硬度感知特性对于对比损失的成功至关重要 同时提出了一个变种$margin-\\tau$损失，通过改变margin损失前面的1/N，将其转换为$\\frac{\\exp(s(t^\\prime_i)/\\tau)}{\\sum_{j=1}^{|\\mathcal{N}|}\\exp(s(t^\\prime_j)/\\tau)}$，其中$s(t^\\prime_i) = \\max(0, \\lambda + \\phi(h,r,t^\\prime_i) - \\phi(h,r,t))$ 与InfoNCE相似,$margin-\\tau$可以更多的关注硬负样本，从而得到更高的性能提升，这与RotatE中的自对抗负采样类似。 ","date":"2022-07-01","objectID":"/simkgc/:6:1","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"图结构重排序 通过重排序，性能会有一定的提升，但重排序并不总是通用的，未来可以考虑使用图神经网络的方式来实现更高效的图结构感知 ","date":"2022-07-01","objectID":"/simkgc/:6:2","tags":["机器学习","深度学习","KG"],"title":"SimKGC","uri":"/simkgc/"},{"categories":["机器学习","深度学习","KG"],"content":"PairRE ","date":"2022-06-09","objectID":"/pairre/:0:0","tags":["机器学习","深度学习","KG"],"title":"PairRE","uri":"/pairre/"},{"categories":["机器学习","深度学习","KG"],"content":"Novelty 既考虑了复杂关系建模的问题，又考虑了多种模式的关系 通过修改RotatE的公式，将关系参数改为头尾两个，从而可以对复杂关系进行建模(RotatE本身就可以对多种模式关系建模) 根据本文分析，RotatE本身可以对N-1关系建模，但是不能对1-N和N-N关系建模 ","date":"2022-06-09","objectID":"/pairre/:1:0","tags":["机器学习","深度学习","KG"],"title":"PairRE","uri":"/pairre/"},{"categories":["机器学习","深度学习","KG"],"content":"问题和动机 KGE模型中存在两个 广泛研究的问题 对复杂关系进行合适建模，如N-1，1-N以及N-N的关系 对不同模式的关系类型进行建模，主要包括，对称/反对称，逆和合成关系。 对称和反对称关系 $r(x,y) \\Rightarrow r(y,x) $ and $r(x,y) \\Rightarrow \\lnot r(y,x)$ 逆关系 $r_2(x,y) \\Rightarrow r_1(y,x)$ 组合关系 $r_1(x,y) \\land r_2(y,z) \\Rightarrow r_3(x,z)$ 现有的方法并不能同时很好的解决这两种问题，如下图所示 TransH，TransR等模型专注于解决复杂关系建模，不能对不同模式的关系建模 RotatE可以对多种模式的关系进行建模，但复杂关系表达能力较弱 ","date":"2022-06-09","objectID":"/pairre/:2:0","tags":["机器学习","深度学习","KG"],"title":"PairRE","uri":"/pairre/"},{"categories":["机器学习","深度学习","KG"],"content":"过去的解决方法 距离模型难以对不同模式的关系建模 TransE将关系解释为平移向量，效率较高，难以建模对称关系，假如某个关系对称，则该关系r的嵌入向量只能为0。同时难以建模复杂关系 TransH，TransR等模型改进了TransE难以建模复杂关系的缺点，但通常用的方法都是将实体投影到关系特定的超平面中，由于投影参数的问题，难以建模逆和合成关系。 RotatE虽然可以对各种模式的关系进行建模，但复杂关系表现能力弱 语义匹配模型，如RESCAL，DisMult，HoIE，ComplEx，QuatE等模型都无法表达组合关系，而且只有嵌入维度满足大于N/32时1，才可以完全表达(N是数据集中的实体数) 其他神经网络模型是黑盒，难以分析 ","date":"2022-06-09","objectID":"/pairre/:3:0","tags":["机器学习","深度学习","KG"],"title":"PairRE","uri":"/pairre/"},{"categories":["机器学习","深度学习","KG"],"content":"idea 将关系表示用两个成对的矩阵表示，分别与头尾实体做哈德曼乘积来对复杂问题进行建模 采用两个成对的矩阵，并采用哈德曼积，可以有效的对多种模式的关系进行建模 采用与距离模型相同的得分函数，并采用RotatE的自对抗负采样损失进行训练 得分函数如下 $$f_r(h,t) = -||h \\circ r^H - t \\circ r^T ||$$ 损失函数如下 \\begin{align*} p((h^\\prime_i,r,t^\\prime_i)|\\{h_i,r_i,t_i\\}) \u0026= \\frac{\\exp f_r(h^\\prime_i,t^\\prime_i)}{\\sum_i \\exp \\alpha f_r(h^\\prime_j,t^\\prime_j)} \\\\ L = \u0026-log\\sigma (\\gamma - f_r(h,t)) \\\\ \u0026-\\sum_{i=1}^{n}p(h^\\prime_i,r,t^\\prime_i)log\\sigma(f_r(h^\\prime_i,t^\\prime_i) - \\gamma) \\end{align*} ","date":"2022-06-09","objectID":"/pairre/:4:0","tags":["机器学习","深度学习","KG"],"title":"PairRE","uri":"/pairre/"},{"categories":["机器学习","深度学习","KG"],"content":"对于多种模式可表达性的证明 对称关系 $$ e_1 \\circ r_1^H = e_2 \\circ r_1^T \\land e_2 \\circ r_1^H = e_1 \\circ r_1^T \\\\ \\Rightarrow r_1^H = r_1^T $$ 逆关系 $$ e_1 \\circ r_1^H = e_2 \\circ r_1^T \\land e_2 \\circ r_2^H = e_1 \\circ r_2^T \\\\ \\Rightarrow r_1^H \\circ r_2^H = r_1^T \\circ r_2^T $$ 组合关系 $$ e_1 \\circ r_1^H = e_2 \\circ r_1^T \\land e_2 \\circ r_2^H = e_3 \\circ r_2^T \\land \\\\ e_1 \\circ r_3^H = e_3 \\circ r_e^T \\\\ \\Rightarrow r_1^T \\circ r_2^T \\circ r_3^H = r_1^H \\circ r_2^H \\circ r_3^T $$ 通过添加简单的约束，PairRE同样可以对子关系进行编码 子关系定义如下 $$ \\forall h,t \\in ^epsilon,(h,r_1,t) \\rightarrow (h,r_2,t) $$ 采用以下约束 $$ \\frac{r_{2,i}^H}{r_{1,i}^H} = \\frac{r_{2,i}^T}{r_{1,i}^T} = \\alpha_i, |\\alpha_i| \\leq 1 $$ 可以推导出 \\begin{align*} \u0026 f_{r2}(h,t) - f_{r1}(h,t) \\\\ \u0026= ||h \\circ r_1^H - t \\circ r_1^T || - ||h \\circ r_2^H - t \\circ r_2^T || \\\\ \u0026= ||h \\circ r_1^H - t \\circ r_1^T || - ||\\alpha (h \\circ r_1^H - t \\circ r_1^T) || \\\\ \u0026 \\geq 0 \\end{align*} 因此可以保证$(h,r_2,t)$更加合理 ","date":"2022-06-09","objectID":"/pairre/:5:0","tags":["机器学习","深度学习","KG"],"title":"PairRE","uri":"/pairre/"},{"categories":["机器学习","深度学习","KG"],"content":"实验 四个数据集上测试性能(PairRE可以在16G显存的GPU上运行，而特殊符号需要在48G显存的GPU上运行) 在带有子关系的数据集上进行测试 Sport数据集是NELL的一个子集，主要关系是反对称和子关系 DB100K是DBpedia的一个子集，主要关系是组合和子关系 DB100K数据集 Sport数据集 对于复杂关系的实验 对RotatE添加成对关系参数（类似于消融实验） 多种模式关系的分析 ","date":"2022-06-09","objectID":"/pairre/:6:0","tags":["机器学习","深度学习","KG"],"title":"PairRE","uri":"/pairre/"},{"categories":["机器学习","深度学习","KG"],"content":"KBGAN模型1 ","date":"2022-06-09","objectID":"/kbgan/:0:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"Novelty 首次使用生成对抗网路的思想，全篇关注负样本的生成质量问题 采用生成对抗网络，采用概率模型作为生成器，采用距离模型作为鉴别器，训练鉴别器 使用强化学习的策略梯度方法来训练生成器 ","date":"2022-06-09","objectID":"/kbgan/:1:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"问题和动机 过去知识图谱嵌入领域的负采样仅仅根据概率来采样有以下两个问题 假负样本问题，有可能会有正确样本被当成负样本来采样 低质量负样本问题，会产生一些质量很低的，对训练意义不大的样本(很容易与正样本区分开的样本，对训练贡献很小) 收到对抗生成网络思想的影响，本文采用生成器来生成高质量负样本，采用鉴别器来计算嵌入 生成器使用基于概率的log-loss损失 鉴别器使用基于距离的margin-loss损失 由于存在离散的生成过程，因此不能使用基于梯度的方法进行优化，本文采用强化学习中的方差减少方法来进行优化。 ","date":"2022-06-09","objectID":"/kbgan/:2:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"相关工作 GAN最早用于生成图像 生成器接受噪声输入并输出图像 鉴别器是一种分类器，将图像分类为真和假。 训练GAN时，生成器试图生成真实图像来欺骗鉴别器，鉴别器试图将其与真是图像区分开。 GAN还能够生成满足特定要求的样本 用于自然语言处理 梯度传播不适用于离散采样的步骤，因此不能使用原始GAN来生成离散样本，如自然语言句子或三元组 SEQGAN使用强化学习来解决离散问题，使用了策略梯度和其他技巧来训练生成器。 鉴别器不一定是分类器。后来很多方法使用回归器作为鉴别器。 ","date":"2022-06-09","objectID":"/kbgan/:3:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"过去的解决方法 过去采用随机替换来生成负样本，但这样容易产生低质量的样本 对于log-softmax损失函数而言，通常会为一个正样本产生很多个负样本，因此总有高质量的负样本可以使用，因此低质量样本的影响不大 论文2(ComplEx)研究表明，当采用100：1的负/正比例会使得效果更佳 但对于margin-loss而言，正负样本通常为1:1，因此低质量负样本会影响训练 ","date":"2022-06-09","objectID":"/kbgan/:4:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"idea 采用一个softmax概率的生成器(这是因为softmax给出的是一组三元组的概率分布，这是生成器从中采样所必须的东西)，来生成高质量的负样本。 采用一个marginal损失函数来构造鉴别器，这类损失可以从高质量的负样本中受益更多。 本模型的最终目标是生成一个好的鉴别器，而GAN的目标是训练一个好的生成器 鉴别器应该为高质量的负样本分配较小的距离 生成器的目标是最小化鉴别器中为生成三元组给出的距离 鉴别器的目标是最小化正三元组和负三元组之间的边际损失。 生成器和鉴别器会针对各自的目标交替训练 ","date":"2022-06-09","objectID":"/kbgan/:5:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"模型 KBGAN框架概述:生成器（G）计算一组候选负三元组的概率分布，然后从分布中抽取一个三元组作为输出。鉴别器（D）接收生成的负三元组和真值三元组（在六角形框中），并计算它们的分数。G通过策略梯度最小化生成的负三元组的得分，D通过梯度下降最小化正三元组和负三元组之间的边际损失 假设给定正三元组$(h,r,t)$，生成器给出的负三元组的概率分布为$p_G(h^\\prime ,r , t^\\prime | h,r,t)$，并通过该分布生成负样本$(h^\\prime,r,t^\\prime)$ 鉴别器的得分函数为$f_D(h,r,t)$ ","date":"2022-06-09","objectID":"/kbgan/:6:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"生成器 生成器中的采样概率为$Neg(h,r,t)\\subset\\{(h^\\prime,r,t)|h^\\prime\\in\\mathcal{E}\\}\\cup\\{(h,r,t^\\prime)|t^\\prime\\in\\mathcal{E}\\}$ \\begin{align*} p_G(h^\\prime,r,t^\\prime|h,r,t)=\\frac{\\exp f_G(h^\\prime,r,t^\\prime)}{\\sum\\exp f_G(h^\\ast,r,t^\\ast)} \\\\ (h^\\ast,r,t^\\ast)\\in Neg(h,r,t) \\end{align*} 理想情况下，$Neg(h,r,t)$应该包含所有的负样本，然而，知识图通常是高度不完整的，因此“最难”的负三元组很可能是假阴性（真实事实）。为了解决这个问题，通过从E中均匀采样Ns实体（与所有可能的负样本相比，这是一个很小的数字）来代替h或t来生成Neg（h，r，t）。因为在现实知识图中，真否定通常远远大于假否定，这样的集合不太可能包含任何假否定，生成器选择的负片很可能是真负样本。使用较小的$Neg(h,r,t)$也可以显著降低计算复杂性。 同时采用伯努利负采样的策略，以更大的概率来替换1对N和N对1中1的那端。 生成器的目标可表述为，最大化以下负距离的期望值 \\begin{align*} R_G\u0026=\\sum_{(h,r,t)\\in\\mathcal{T}}\\mathbb{E}[-f_D(h^\\prime,r,t^\\prime)] \\\\ \u0026(h^\\prime,r,t^\\prime)\\sim p_G(h^\\prime,r,t^\\prime|h,r,t) \\end{align*} 此生成器有一个离散采样的步骤，因此无法直接用简单的微分求出梯度 采用策略梯度理论来生成上述公式的梯度 \\begin{align*} \\nabla_G R_G\u0026=\\sum_{(h,r,t)\\in T}E_{(h^\\prime,r,t^\\prime)\\sim p_G(h^\\prime,r,t^\\prime|h,r,t)} \\\\ \u0026[-f_D(h^\\prime,r,t^\\prime)\\nabla_G \\log p_G(h^\\prime,r,t^\\prime|h,r,t)] \\\\ \u0026\\simeq \\sum_{(h,r,t)\\in\\mathcal{T}}\\frac{1}{N}\\sum_{(h_i^\\prime,r,t_i^\\prime)\\sim p_G(h^\\prime,r,t^\\prime|h,r,t), i=1\\dots N} \\\\ \u0026[-f_D(h^\\prime,r,t^\\prime)\\nabla_G \\log p_G(h^\\prime,r,t^\\prime|h,r,t)] \\end{align*} ","date":"2022-06-09","objectID":"/kbgan/:6:1","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"鉴别器 鉴别器的目标可表述为最小化如下损失函数 \\begin{align*} L_D\u0026=\\sum_{(h,r,t)\\in\\mathcal{T}}[f_D(h,r,t)-f_D(h^\\prime,r,t^\\prime)+\\gamma]_+ \\\\ \u0026(h^\\prime,r,t^\\prime)\\sim p_G(h^\\prime,r,t^\\prime|h,r,t) \\end{align*} ","date":"2022-06-09","objectID":"/kbgan/:6:2","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"强化学习的角度 生成器可以看作一个agent，通过执行操作与环境交互，通过最大化环境的响应回报来改进自身 鉴别器可以看作environment 具体而言$(h,r,t)$为状态，$p_G(h^\\prime,r,t^\\prime|h,r,t)$为策略，$(h^\\prime,r,t^\\prime)$为行动，$-f_D(h^\\prime,r,t^\\prime)$为汇报。 在典型的强化学习中，动作的每个操作都要改变状态，知道达到特定状态或者操作数达到限制。但本文中的方法不改变状态。因此每个epoch只包含一个行动。 在强化学习中，为了减少强化算法的方差，通常会在奖励中减去一个baseline，在本文中，我们将$-f_D(h^\\prime,r,t^\\prime)$替换为$-f_D(h^\\prime,r,t^\\prime)-b(h,r,t)$，为了不引入别的参数，设b为$b=\\sum_{(h,r,t)\\in\\mathcal{T}}\\mathbb{E}_{(h^\\prime,r,t^\\prime)\\sim p_G(h^\\prime,r,t^\\prime|h,r,t)}[-f_D(h^\\prime,r,t^\\prime)]$ ","date":"2022-06-09","objectID":"/kbgan/:6:3","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"算法 ","date":"2022-06-09","objectID":"/kbgan/:7:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"实验 为了验证框架的可行性，本论文选择了不同的生成器和鉴别器，并验证了所有的可能组合 生成器选择了两个基于概率的模型DISTMULT和CompLEx 鉴别器选择了两个经典的基于翻译的模型TransE和TransR 选择了FB15K-237和WN18以及WN8RR三个数据集来验证性能，结果如下 生成器的选择(DISTMULT或者CompLEx)不会显著影响实验结果，KGGAN框架可以提升Trans系列模型的性能 未来可以使用更先进的生成器或者鉴别器来优化模型性能 该模型在训练过程中性能始终处于上升状况，如下图所示 KBGAN: Adversarial Learning for Knowledge Graph Embeddings ↩︎ Complex Embeddings for Simple Link Prediction ↩︎ ","date":"2022-06-09","objectID":"/kbgan/:8:0","tags":["机器学习","深度学习","KG"],"title":"KBGAN","uri":"/kbgan/"},{"categories":["机器学习","深度学习","KG"],"content":"CAKE模型1 ","date":"2022-05-25","objectID":"/cake/:0:0","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"Novelty 首次既考虑了假阴性问题，又考虑了低质量的问题，特别是假阴性问题几乎无人考虑 将数据集做一定的处理，进行标注，为每个实体赋予一到多个概念 基于概念生成常识，也即（概念，关系，概念）的三元组为常识1和（概念组，关系，概念组）为常识2 使用域采样的假设，分别对1-N中1和N中的采用不同的采样方式，首先选取概念，然后从概念组中生成负样本 最后链路预测选择实体时，选择符合常识1的概念中的实体 ","date":"2022-05-25","objectID":"/cake/:1:0","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"问题和动机 本论文主要关注KGE中存在的问题 KGE训练时需要对训练数据进行负采样，简单的根据概率来生成负样本，会有如下两个问题 假负样本问题，有可能会有正确样本被当成负样本来采样 低质量负样本问题，会产生一些质量很低的，对训练意义不大的样本(很容易与正样本区分开的样本，对训练贡献很小) 过去的知识图谱补全仅仅依赖于现有的知识图谱本身，是数据驱动的 进行链路预测的时候有可能根据现有的知识图谱数据预测出错误的结果 如果根据常识来看，就能很明显的排除掉错误的预测结果 现有的一些大规模常识KG，如ConceptNet只包含概念，没有与之对应的实体链接，因此无法用于KGC任务 本文解决上述两个问题的思路如下 首先从知识图谱中构建一个显示的常识 基于常识来采样，克服负采样的问题，解决假负样本和低质量负样本问题 构建多视图的链路预测 确定常识试图中属于正确概念的实体候选集 再通过传统的KGE来预测正确的答案 ","date":"2022-05-25","objectID":"/cake/:2:0","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"过去的解决方法 ","date":"2022-05-25","objectID":"/cake/:3:0","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"负采样 过去的负采样方法主要分为以下五类 随机均匀采样:从均匀分布中随机替换正确三元组中的实体或者关系来生成负样本，包括 最早的随机破坏来生成负样本 TransH2引入一个伯努利概率$\\frac{t_{ph}}{t_{ph}+h{pt}}$来处理一对多，多对一问题的概率采样，其中$t_{ph}$和${h_{pt}}$分别代表每个头实体对应的尾实体的平均数，每个尾实体对应的头实体的平均数 基于域的采样:基于域的采样假设负采样时，与关系关联的头尾实体只能属于特定的域，若有过多的负样本位于该域之外时，会在真实三元组和被破坏的三元组之间产生较大的能量差，当能量差过大时难以对训练产生有效的影响。 基于域的采样基于局部封闭世界假设LCWA，该假设在此的主要应用为对域进行定义，某个关系连接的实体属于同一个域 3采用域采样，对某个三元组从同域内生成负样本，也即替换h和r时从同域里查找替换 4对3做了改进，使用一个概率$p_r = min ( \\frac {\\lambda |M_r^T||M_r^H|}{|N_r|},0.5)$从域内采样，使用$1-p_r$从域外采样，其中$N_r = \\{(h,r,t) \\in P \\}$，$M_r^H = \\{h | \\exists h(h,r,t) \\in P \\}$，$M_r^T$与之对应。 为何要采用上述的$p_r$是根据以下假设 假设$O_r$是关系$r$域中所有正确的三元组，那破坏后产生的三元组是正确三元组的概率为$\\frac{|O_r|}{|M_r^H||M_r^T|}$， $|N_r| = \\lambda |O_r|$，因此概率为$p_r = \\frac {|N_r|}{\\lambda |M_r^T||M_r^H|}$ 生成负样本的概率应该与样本为真的概率成反比，因此得到上述等式 $\\lambda$通常设置的很小，0.001左右，其值越小MR越小，其值越大H值越大，但生成有效三元组作为负样本的概率更高了，所以总体秩有所上升 基于对抗的采样:该种采样方式基于生成对抗网络的思想。 KBGAN5在KGE模型中使用GAN，负采样作为生成器，KGE模型作为鉴别器，在对抗训练中缠上高质量的负三元组。 RotatE6改进了KBGAN的方法，因为生成对抗网络难以训练，复杂度过高，因此RotatE将生成器的一部分作为概率来对负样本进行抽样，改进损失函数，使其更相似于KBGAN中的鉴别器，从而达到了自对抗的效果。(最常用的方法) 高效采样:7使用包含负三元组候选的缓存来提高采样效率 无采样:8通过将KGE的损失函数转换为同一的平方损失，消除了负采样的过程 前四种模型的都是尝试以更高的概率对损坏的三元组来进行采样，这些三元组可能是正确的但在知识图谱中丢失掉的三元组 基于域的负采样严重依赖于单一类型的约束，限制了负三元组的多样性 基于对抗的采样引入了GAN，使得原始的模型更复杂且难以训练(RotatE中引入的自对抗可以解决复杂度问题) 无采样必须将每个原始KGE模型转换为平方损失，会削弱KGE模型的性能 ","date":"2022-05-25","objectID":"/cake/:3:1","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"常识图谱 很多常识图谱，如ConceptNet,Microsoft Concept Graph,ATOMIC等都只包含概念，没有相应的实体链接，所以不适合KGC 其他一些KGC模型，如JOIE采用了KG中内置的本体，即NELL和DBpedia，但本题中的关系都是层次关系，不是明确的常识，对于KGC来说用处不大，事实关系和本体关系没有重叠。 ","date":"2022-05-25","objectID":"/cake/:3:2","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"idea 本文解决上述两个问题的思路如下 首先从知识图谱中构建一个显示的常识，自动尝试生成模块(ACG)，从事实三元组和实体概念中抽取尝试 基于常识来采样，克服负采样的问题，解决假负样本和低质量负样本问题，常识感知负采样模块(CANS)，利用生成的常识生成高质量的负三元组。 构建多视图的链路预测 确定常识试图中属于正确概念的实体候选集，MVLP模块从粗到细的方式进行链路预测，从常识的角度对候选对象进行过滤 再通过传统的KGE来预测正确的答案 ","date":"2022-05-25","objectID":"/cake/:4:0","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"模型 ","date":"2022-05-25","objectID":"/cake/:5:0","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"ACG模块 人工标注了实体对应的概念，开源代码里有新的数据集 将常识分为两类，第一种C1是概念三元组，第二种是C2相同概念组成的三元组集合. 直接使用概念来替换实体，从而生成常识 第一种常识是通过消除重叠的概念级别三元组来实现 第二种常识是将包含相同关系的概念级三元组合并到一个集合里 ","date":"2022-05-25","objectID":"/cake/:5:1","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"CANS模块 该模块根据N-1的三元组有以下假设 对于1的那一端，损坏三元组肯定是实际的负三元组。同时，基于域采样的原理，与正确实体共享至少一个概念的损失三元组是高质量的负三元组。 对于n的那一端，与正确实体位于同一概念的实体更可能是假的负三元组，因此在训练过程中同一概念中的这些三元组采样的权重应该尽可能低。同时应尽可能在C2中进行采样，因为根据域采样的假设，可以获得更高质量的负三元组。 可以看出，对于采样而言，首先考虑假阴性问题，而后考虑高质量负样本的问题，基于RotatE，采样概率越高，则质量越高，但对于N的那一端，质量高就意味着更容易出现假阴性问题，因此1-采样概率，而对于1的那端不需要考虑假阴性，因此就用采样概率生成更高质量的负样本 详细来讲 基本分为两步，第一步选取概念，第二步从概念到实体的转换 对于1的那一端，首先生成与1相同概念的三元组集合，然后通过以下概率来对这些三元组集合进行采样 \\begin{align*} w(h,r,t^\\prime_i) \u0026= p((h,r,t^\\prime_i)|\\{h_i,r_i,t_i\\}) \\\\ \u0026= \\frac{\\exp \\alpha E(h,r,t^\\prime_i)}{\\sum_i \\exp \\alpha E(h,r,t^\\prime_i)} \\end{align*} 对于N的那一端，根据C2，生成一组概念，然后对这组概念里的三元组采用以下概率采样 \\begin{align*} w(h^\\prime_j,r,t) \u0026= 1 - p((h^\\prime_j,r,t)|\\{h_i,r_i,t_i\\}) \\\\ \u0026= 1 - \\frac{\\exp \\alpha E(h^\\prime_j,r,t)}{\\sum_i \\exp \\alpha E(h^\\prime_i,r,t)} \\end{align*} ","date":"2022-05-25","objectID":"/cake/:5:2","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"MVLP模块 在粗预测阶段，从尝试的角度挑选实体 具体来说，当查询$(h_i,r,?)$时，常识C1用来过滤尾部实体的合理概念，具体候选实体定义为如下 $$C_{1t}=\\{c_{ti}|(c_{hi},r,c_{ti})\\in C_1\\}$$ 属于上述概念集的实体因为满足常识，所以确定为候选实体 最后通过如下公式依次输入所有候选实体，最后选出得分最高的候选实体(因为该模型独立于得分函数，所有可以使用任意的得分函数来进行最后的计算) $$score(e_i) = E(h,r,e_i)$$ ","date":"2022-05-25","objectID":"/cake/:5:3","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"实验 ","date":"2022-05-25","objectID":"/cake/:6:0","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"数据集 FB15K和FB15K-237，每个实体总是属于多个概念 NELL-955和DBpedia-242每个实体只有一个概念 ","date":"2022-05-25","objectID":"/cake/:6:1","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"添加CAKE各个模块后的性能对比 本模型在四个数据集上，测试了TransE，RotatE和HAKE三个模型分别添加CAKE各个模块的性能，结果如下图 可以发现两个模块在大多数情况下均对模型性能有一定的提升 ","date":"2022-05-25","objectID":"/cake/:6:2","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"与其他负采样方法的对比 论文同样比较了CANS中负采样和其他几种典型负采样的性能，结果如下图所示 可以发现CANS中的负采样相比自对抗负采样一定的性能提升 除此之外自对抗采样在除FB15K-237数据集之外，性能都达到了第二的水平 FB15K-237数据集更多关注复杂关系，因此域采样达到了第二的水平 ","date":"2022-05-25","objectID":"/cake/:6:3","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["机器学习","深度学习","KG"],"content":"消融实验 -CRNS代表去掉复杂关系，保留常识部分，即不对1-N中的1和N采样时加以区分 -CSNS代表去掉常识，保留复杂关系，即去掉C2部分 -MVLP代表去掉MVLP模块 下图可以看出对于不同的数据集，不同模块都可以起到一定的作用 CAKE: A Scalable Commonsense-Aware Framework For Multi-View Knowledge Graph Completion(ACL 2022) ↩︎ Knowledge graph embedding by translating on hyperplanes(AAAI 2014) ↩︎ Type-constrained representation learning in knowledge graphs(ISWC 2015) ↩︎ ↩︎ An interpretable knowledge transfer model for knowledge base completion(ACL 2017) ↩︎ KBGAN: Adversarial learning for knowledge graph embeddings(NAACL 2018) ↩︎ RotatE: Knowledge graph embedding by relational rotation in complex space(ICLR 2019) ↩︎ NSCaching:Simple and efficent negative sampling for knowledge graph embedding(ICDE 2019) ↩︎ Efficient non-sampling knowledge graph embedding(WWW 2021) ↩︎ ","date":"2022-05-25","objectID":"/cake/:6:4","tags":["机器学习","深度学习","KG"],"title":"CAKE","uri":"/cake/"},{"categories":["编程语言"],"content":"Python学习笔记 ","date":"2022-04-02","objectID":"/python/:0:0","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.python基础 ","date":"2022-04-02","objectID":"/python/:1:0","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.1python语法常识 python语法采用缩进格式而不是括号来进行代码块分割的 缩进没有空格个数或者tap键的约束，但应坚持使用四个空格缩进 当语句以:结尾时，缩进的语句视为代码块 以#开头的是注释 复制粘贴功能基本等于失效，粘贴的代码必须检查缩进是否正确 python是大小写敏感的，写错了大小写，程序会报错 #print a = 100 if a \u003e= 0: print(a) else: print(-a) ","date":"2022-04-02","objectID":"/python/:1:1","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.2基本数据类型 1.2.1数据类型分类 1.2.1.1数字型与非数字型 python中的变量一般分为两种类型 数字型和非数字型 数字型 整型 浮点型 布尔型 复数型 非数字型 字符串 列表（list） 元组(tuple) 字典(dict) 1.2.1.2不可变类型与可变类型 python中变量都是通过对数据的引用来定义的。 不可变类型只能在内存中重建一组数据，然后让变量指向数据 可变类型直接可以改内存中定义好的数据 给变量赋新值是改变变量指向的地址 通过方法改变数据的值不影响变量指向的地址 不可变类型 数字类型 字符串 元组 可变类型 列表 字典 哈希函数hash 接收一个不可变类型作为参数 返回值是一个整数 python中设置字典键值对时，会首先对key进行hash()决定数据如何在内存中保存，方便之后的增删改查 键值对的key必须是不可变类型 键值对的value可以是任何类型 1.2.2基本数据类型介绍 python是动态语言变量本身类型不固定 整数 十六进制前面用**0x**例0xff00 python的整数和浮点数没有大小限制 /结果是精确的，//是除法结果取整，%是取余 浮点数 1.23*10^9等于1.23e9 浮点数有误差 字符串 用''或\"\"扩起来的任意文本 转义字符\\ 可以用r''表示''内部的字符串默认不转义 字符串内部有多个换行，写在\\中不好阅读'''...'''表示多行内容 ...是提示符，不是代码的一部分，不需要手打 print('''line1 ...line2 ...line3''') line1 line2 line3 print(r'''hello,\\n world''') hello,\\n world 布尔 True False and or not运算 3\u003e2输出True 空值 None表示空值 常量 变量 python是动态语言 首先在内存中创建值，然后将变量指向该值 x = y是把变量x指向真正的对象，y改变不影响x的值 ","date":"2022-04-02","objectID":"/python/:1:2","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.3运算符 1.3.1算术运算符 运算符 描述 ** 乘幂 + 加法 - 减法 / 除法 * 乘法 // 取整数 % 取余 *还可用于字符串，结果是将运算符重复n次 运算符优先级与C语言类似。可以通过()来增加优先级 1.3.2赋值运算符 运算符 描述 实例 = 赋值 c = a += 加法赋值 c += a c = c + a -= 减法赋值 c -= a c = c - a /= 除法赋值 c /= a c = c / a *= 乘法赋值 c *= a c = c * a //= 取整数赋值 c //=a c = c //a %= 取余赋值 c %= a c = c % a **= 幂赋值 c **= a c = c ** a 1.3.3比较运算符 运算符 描述 举例 == 等于判断 a==b != 不等于判断 a!=b \u003e 大于判断 a\u003eb \u003c 小于判断 a\u003cb \u003e= 大于等于判断 a\u003e=b \u003c= 小于等于判断 a\u003c=b 结果为True和False 1.3.4逻辑运算符 运算符 描述 举例 and if x is false, then y, else x a and b or if x is false, then x, else y a or b not if x is false, then True, else False not a 1.3.5位运算符 运算符 描述 举例 | 按位与 a | b ^ 按位异或 a ^ b \u0026 按位与 a\u0026b « 左移 a«b » 右移 a»b ~ 按位取反 ~a ","date":"2022-04-02","objectID":"/python/:1:3","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.4字符串和编码 1.4.1字符串和编码总览 ASCII编码是一个字节，表示数量有限 Unicode编码是两个字节,但是空间多一倍 UTF-8可变长，把一个Unicode字符编码成1-6个字节。英文是一个字节，汉字通常三个字节，大量英文就能升空间。 本地计算机中存储用UTF-8，内存中用Unicode，需要时互相转换 浏览网页,服务器Unicode,转换成UTF-8到网页 1.4.2python的字符串 python字符串是以Unicode来编码的 ord('A')可获取字符的整数表示 \u003e\u003e\u003e ord('A') 65 \u003e\u003e\u003e ord('中') 20013 **chr(66)**把整数表示转换为对应的字符 \u003e\u003e\u003e chr(66) 'B' \u003e\u003e\u003e chr(25991) '文' .encode() 将字符串用特定编码方式转换为字节输出。 **.decode**将字节读入的字符串按特定编码方式输出 \u003e\u003e\u003e 'ABC'.encode('ascii') b'ABC' \u003e\u003e\u003e '中文'.encode('utf-8') b'\\xe4\\xb8\\xad\\xe6\\x96\\x87' \u003e\u003e\u003e b'ABC'.decode('ascii') 'ABC' \u003e\u003e\u003e b'\\xe4\\xb8\\xad\\xe6\\x96\\x87'.decode('utf-8') '中文'.encode('ascii') #报错 \u003e\u003e\u003e '中文'.encode('ascii') \u003e\u003e\u003e b'\\xe4\\xb8\\xad\\xff'.decode('utf-8') #忽视少数字符 \u003e\u003e\u003e b'\\xe4\\xb8\\xad\\xff'.decode('utf-8',errors='ignore') '中' len()计算字符串的字符数,如果是字节表示的，就计算字节数 \u003e\u003e\u003e len(b'ABC') 3 \u003e\u003e\u003e len(b'\\xe4\\xb8\\xad\\xe6\\x96\\x87') 6 \u003e\u003e\u003e len('中文'.encode('utf-8')) 6 \u003e\u003e\u003e len('ABC') 3 \u003e\u003e\u003e len('中文') 2 python文本文件储存和读取格式 指定python读取格式 #！/usr/bin/env python3 # -*- coding: utf-8 -*- 指定python储存格式 储存格式需要通过编辑器指定 python字符串格式化的第一种形式 与C语言一致，通过%d%f%s%x指定 \u003e\u003e\u003e 'hello,%s' % 'world' 'hello,world' \u003e\u003e\u003e 'Hi,%s,you have $%d'%('Michael',1000) 'Hi,Michael,you have $1000' 不知道应该用什么,%s永远起作用 python字符串格式化的第二种形式 \u003e\u003e\u003e 'Hello,{0},成绩提升了 {1:.1f}%'.format('小明',17.125) 'Hello,小明，成绩提升了 17.1' \u003e\u003e\u003e str = 'Hello,{0},成绩提升了 {1:.1f}%' \u003e\u003e\u003e print(str.format('小明',17.125)) 'Hello,小明，成绩提升了 17.1' ","date":"2022-04-02","objectID":"/python/:1:4","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.5条件判断语句 if salary \u003e= 10000: print('happy') elif salary \u003e= 5000: print('common') else: print('sad') ","date":"2022-04-02","objectID":"/python/:1:5","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.6循环语句 #for循环 sum = 0 for x in [1,2,3,4,5,6,7,8,9]: sum = sum + x print(sum) #range函数生成证书序列，和list()函数转换为list \u003e\u003e\u003e list(range(5)) [0,1,2,3,4] #举例 sum = 0 for x in range[101]: sum = sum + x print(sum) else: 没有通过nreak跳出循环，循环结束后会执行的代码 5050 #while循环 sum = 0 n = 99 while n \u003e 0 : sum = sum + n n = n-2 print(sum) ##break和continue两函数与C语言用法一样 ","date":"2022-04-02","objectID":"/python/:1:6","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"1.7高级数据类型 1.7.1变量的分类与简介 1.7.1.1数字型与非数字型 python中的变量一般分为两种类型 数字型和非数字型 数字型 整型 浮点型 布尔型 复数型 非数字型 字符串 列表（list） 元组(tuple) 字典(dict) 非数字变量的特点 都可以看成一个序列（sequence),也可以理解为容器 取值[] 遍历for x in 计算长度、大小，最大小值、比较，删除 链接+和重复* 切片 1.7.1.2可变类型与不可变类型 python中变量都是通过对数据的引用来定义的。 不可变类型只能在内存中重建一组数据，然后让变量指向数据 可变类型直接可以改内存中定义好的数据 给变量赋新值是改变变量指向的地址 通过方法改变数据的值不影响变量指向的地址 不可变类型 数字类型 字符串 元组 可变类型 列表 字典 哈希函数hash 接收一个不可变类型作为参数 返回值是一个整数 python中设置字典键值对时，会首先对key进行hash()决定数据如何在内存中保存，方便之后的增删改查 键值对的key必须是不可变类型 键值对的value可以是任何类型 1.7.1.3局部变量与全局变量 局部变量在函数内部定义，只在*该函数内部使用** 全局变量在函数外部定义,所有函数都可以使用该变量 大多数语言不推荐使用全局变量 函数在使用变量时，会首先检索局部变量，如果没有局部变量，会检索全局变量 1.7.1.3.1局部变量 局部变量是在函数内部定义的变量，只有在该函数内部才可以使用 函数执行结束后，函数内部的局部变量会被系统回收 不同函数中局部变量可以重名，且无影响 局部变量的生命周期 生命周期指从被创建到被系统回收的过程 局部变量在函数执行时才被创建，函数执行结束后被回收 局部变量在生命周期内，可以用来保存函数内部使用到的数据 1.7.1.3.2全局变量 全局变量是在函数外部定义的变量，所有函数都可以使用 默认函数内部不允许直接修改全局变量的引用,使用赋值语句修改全局变量的值 如果使用使用赋值语句修改全局变量的值，会重新定义一个局部变量 要在函数内部修改全局变量的名，需要加global关键字声明变量 应将全局变量定义在所有函数的上方 num = 10 def demo1(): global num num = 20 #这样可以修改全局变量 #global关键字会告诉解释器，后面的值是一个全局变量， #再使用赋值语句时，不会再创建局部变量 1.7.2集合类型list(列表)和tuple（元组） 1.7.2.1list(类似于空类型集合) 列表支持的方法 #取值,索引超出范围会报错 list[0] #取索引，数据不在列表内会报错 list.index('') #修改数据,索引超出范围会报错 list[1] = '李四' #增加数据，append会插到最后,insert会插到指定索引前面,extend可以把一个列表追加到当前列表末尾 #列表中使用+=运算符会直接调用列表的extend()方法 #相加再赋值,并不会调用extend()方法 list.append('李四') list.insert(1,'李四') list.extend(name_list) #删除数据，remove会删除第一个出现的元素，元素不存在时，会报错。clear会把列表清空，pop默认把列表最后一个元素删除，同时可以指定删除元素的索引.建议使用前三个方法。del关键字本质上是将一个变量从内存中删除 list.remove('李四') list.pop(3) list.clear() del list[0] #统计,len统计列表元素个数，count统计某个元素出现的个数 len(list) list.count('李四') #列表排序，sort升序排列，sort(reverse=True),list.reverse()降序排列 list.sort() list.sort(reverse=True) list.reverse() list定义方法(类似于可变数组) classmates = ['a','b','c'] list访问方式（与数组类似） #第一种，正序访问 classmates[0] classmates[1] classmates[2] #第二种，逆序访问 classmates[0] = classmates[-3] classmates[2] = classmates[-1] list追加元素到末尾 \u003e\u003e\u003e classmates.append('d') \u003e\u003e\u003e classmates ['a','b','c','d'] list把元素插入特定位置 \u003e\u003e\u003e classmates.insert(1,'d') \u003e\u003e\u003e classmates ['a','d','b','c','d'] list删除元素 \u003e\u003e\u003e classmates.pop() 'd' \u003e\u003e\u003e classmates ['a','d','b','c'] \u003e\u003e\u003e classmates.pop(1) 'd' \u003e\u003e\u003e classmates ['a','b','c'] list改变元素 \u003e\u003e\u003e classmates[1] = 'd' \u003e\u003e\u003e classmates ['a','d','c'] list元素类型可以是任意的，也可以不同 \u003e\u003e\u003e L = ['a',123,True] \u003e\u003e\u003e s = ['a','b',['c','d'],'d'] \u003e\u003e\u003e len(s) 4 #等同于以下 \u003e\u003e\u003e p = ['c','d'] \u003e\u003e\u003e s = ['a','b',p,'d'] \u003e\u003e\u003e len(s) 4 #预访问C，可采用一下两种形式 \u003e\u003e\u003e p[0] \u003e\u003e\u003e s[2][0] #空list \u003e\u003e\u003e L = [] \u003e\u003e\u003e len(L) 0 list使用场景 将多个同类型的元素保存到list里，遍历处理 list类似于Java的空类型集合list len()函数可以获得list元素的个数 1.7.2.2tuple(类似与数组，一旦初始化就不能改变) tuple支持的方法 #获取索引 info.index('李四') #获取某数据出现的次数 info.count('李四') #统计元素格式 len(info) tuple定义只有一个元素的该类型时，需加一个, \u003e\u003e\u003e t = (1,) \u003e\u003e\u003e t (1,) #否则会产生这样的歧义 \u003e\u003e\u003e t = (1) \u003e\u003e\u003e t 1 #定义空元组 t = () tuple虽然不可变，但是包含的List可以变，因为包含的list是以指针指向的形式保存的,list是可变的 \u003e\u003e\u003e t = (0,1,['A','B']) \u003e\u003e\u003e t[2][0] = 'X' \u003e\u003e\u003e t[2][1] = 'Y' \u003e\u003e\u003e t (0,1,['X','Y']) 元组使用场景 函数多个参数和多个返回值，以元组实现 格式字符串（%d,%s），以元组实现 print('%s, %d' % ('李四'，14)) #等于 info_tuple = ('李四'，14) print('%s，%d' % info_tuple) #等于 info_str = '%s,%d' % info_tuple print(info_str) 让列表数据不被修改tuple(list) 修改tuple，可list(tuple)，修改之后的list，然后再tuple(list) 1.7.3使用dict和set 1.7.3.1dict key只能使用字符串，数字或者元组 字典的常用操作 #取值,如果key值不存在，则会报错 person_dict['key'] #增加'key'不存在，则增加 persion_dict['key'] = 18 #修改，'key'存在，则修改 persion_dict['key'] = 20 #删除 persion_dict.pop('key') #统计键值对的数量 len(person_dict) #合并字典,被合并的字典中包含已经存在的键值对，会覆盖原有的键值对 person_dict.update(dict) #清空 persion_dict.clear() #循环遍历 #k是每一次循环中，获取的键值对的key for k in persion_dict : print('%s - %s' % (k,persion_dict[k])) dict类似与Java中的map 通过key来映射value,所以key的值只能是不可变量，不能是list类型 字典无序，列表有序 创建和使用 \u003e\u003e\u003e d = {'a' : 95,'b' : 75,'c' : 85} \u003e\u003e\u003e d['a'] 95 放入数据 \u003e\u003e\u003e d['d'] = 67 \u003e\u003e\u003e d['d'] 67 改变数据 \u003e\u003e\u003e d['a'] = 90 \u003e\u003e\u003e d['a'] 90 查询是否存在 #方法一 \u003e\u003e\u003e 'e' in d False #方法二,如果不存在返回none或者指定的值，none在交互环境下不显示结果 \u003e\u003e\u003e d.get('e') \u003e\u003e\u003e d.get('e',-1) -1 删除数据 \u003e\u003e\u003e d.pop('a') 90 \u003e\u003e\u003e d {'b' : 75,'c' : 85,'d' : 67} 使用场景 将多个字典放在一个列表里，循环遍历处理 1.7.3.2set set类似与java中的set 创建（需要通过list创建) \u003e\u003e\u003e s = set ([1,2,3,2,3]) \u003e\u003e\u003e s {1,2,3} 添加 s.add(4) 删除 s.remove(4) 交并 \u003e\u003e\u003e s1 = set([1,2,3]) \u003e","date":"2022-04-02","objectID":"/python/:1:7","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"2.函数 定义函数 #普通定义 def my_abs(x): if x \u003e= 0: return x else: return -x #空语句,可以作为占位符 pass #空函数 def empty(): pass #当传入参数个数不对时，会报错 #手动参数检查 def my_abs(x): if not isinstance(x,(int,float)): raise TypeError('bad operand type') if x \u003e= 0: return x else: return -x #返回多个值 import math def mov(x,y): nx = x + 1 ny = y + 1 return nx,ny #函数的返回值实际上是tuple类型 函数参数 在python中类型属于对象，变量没有对象 a = [1,2,3]中,[1,2,3]是List类型，而a没有类型 a仅仅是对象的引用(一个指针) 在python中类型分为可变类型和不可变类型 可变类型：list,dict等 不可变类型：None,Strings,numbers,tuples等 不可变类型赋新值时是创建一个新对象，然后抛弃原来的对象 a = 10然后a = 5，实际上是新创建一个对象5，让a指向5，丢掉原来的10 可变类型直接更改对象 la = [1,2,3]然后la[2] = 4，实际上是对象[1,2,3]变成[1,2,4] #位置参数 def power(x): return x*x #调用 power(3) def power(x,n): s = 1 while n \u003e 0: n = n-1 s = s * n return s #调用 power(3,4) #默认参数 #必选参数在前，默认参数在后，否则会报错,多个参数时，将变化大的放前面，变化小的放后面 #默认参数没有按默认顺序输入时，需要把参数名写上 def power(x,n=2) s = 1 while n \u003e 0: n = n - 1 s = s * n return s #调用 power(3)或power(3,4) def enroll(name, gender, age=6, city='Beijing'): print('name:', name) print('gender:', gender) print('age:', age) print('city:', city) #调用 enroll('Sarah', 'F') 或 enroll('Sarah', 'F'，7)或 enroll('Sarah', 'F'，city = 'TianJin') #可变参数(可参照C语言的指针) def changeme( mylist ): \"修改传入的列表\" mylist.append([1,2,3,4]); print(\"函数内取值: \", mylist) return mylist = [10,20,30]; changeme( mylist ); print(\"函数外取值: \", mylist) #结果 函数内取值: [10, 20, 30, [1, 2, 3, 4]]，函数外取值: [10, 20, 30, [1, 2, 3, 4]] #将其中可变参数变为不可变参数 def changeme( mylist = None): \"修改传入的列表\" mylist.append([1,2,3,4]); print(\"函数内取值: \", mylist) return mylist = [10,20,30]; changeme( mylist ); print(\"函数外取值: \", mylist) #结果 函数内取值: [10, 20, 30, [1, 2, 3, 4]]，函数外取值: [10, 20, 30] #可变参数使用 def calc(numbers): sum = 0 for n in numbers: sum = sum + n * n return sum #此方式必须传入list或者dict def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum #此方式可直接如下调用calc(1,2,3,4) 如果有num = [1,2,3]，那么直接调用calc(*num) #关键字参数(用于传递dict，传入时可以省略) #传入任意个数时 def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw） \u003e\u003e\u003e person('Bob', 35, city='Beijing') name: Bob age: 35 other: {'city': 'Beijing'} \u003e\u003e\u003e person('Adam', 45, gender='M', job='Engineer') name: Adam age: 45 other: {'gender': 'M', 'job': 'Engineer'} \u003e\u003e\u003e extra = {'city': 'Beijing', 'job': 'Engineer'} \u003e\u003e\u003e person('Jack', 24, **extra) name: Jack age: 24 other: {'city': 'Beijing', 'job': 'Engineer'} #限制关键字参数名字 def person(name, age, *, city, job): print(name, age, city, job) ","date":"2022-04-02","objectID":"/python/:2:0","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"2.1函数参数和返回值 函数根据有没有参数和返回值有四种分类 有参数，有返回值 无参数，有返回值 有参数，无返回值 无参数，无返回值 1.处理外部数据，加参数 2.需要向外部汇报结果，返回数据，加返回值 2.1.1函数的返回值 仅返回一个返回值return a result = demo() 返回多个返回值return a1,a2 方式一result = demo(),result是tuple元组类型,单独取值需result[0] 方式二b1,b2 = demo(),此时b1 = a1,b2 = a2 使用多个变量接收返回值时，变量的个数应该与返回值个数一样 否则会报错 扩展，交换两个变量 c = a a = b b = c a = a + b b = a - b a = a - b #仅python可用的方法 a,b = b,a 2.1.2函数的参数 函数参数本质上是在函数内部建立的不能全局化的局部变量(数据引用) 可变参数和不可变参数 如果在函数内部，在函数内部对变量重新赋值，会创建一个新的局部变量(注意与全局变量和局部变量区分开,函数参数与全局变量一般名字不同，如果名字相同，使用global也会报错) 如果在函数内部，对可变类型调用方法改变可变类型的值，会影响数据本来(外部）的值 列表的+=是调用extend()方法 列表的先相加再赋值，不是调用extend()方法 2.1.2.1默认参数(缺省参数) 给某个参数指定一个默认值,具有默认值的参数叫做缺省参数 调用时没有传入缺省参数的值时，会使用默认值 使用默认参数（缺省参数），可以简化函数,例如sort()函数 需要使用最常见的值作为默认值 如果值不能确定，则不能使用默认参数 注意事项 默认参数应放在所有参数的最后面，即，在默认参数后面，不允许出现未设置默认值的参数，否则会报错 默认参数没有按默认顺序输入时，需要把参数名写上,例 def enroll(name, gender, age=6, city='Beijing') 调用enroll('Sarah', 'F'，city = 'TianJin') 或enroll('Sarah', 'F'，7) 2.1.2.2多值参数(可变参数) 有两种情况 参数前加*，可以传递元组,或者列表,函数将传进去的元组和列表统一变成元组处理 元组列表拆包，将一个元组或列表变成一组数据，在元组或列表前加* def calc(*numbers): sum = 0 for n in numbers: sum = sum + n * n return sum #此方式可直接如下调用calc(1,2,3,4) 如果有num = [1,2,3]，那么直接调用calc(*num) 参数前加**，可以传递字典 字典拆包，将一组字典变成一组数据，在字典前加** def person(name, age, **kw): print('name:', name, 'age:', age, 'other:', kw） person('Bob', 35, city='Beijing') #结果name: Bob age: 35 other: {'city': 'Beijing'} person('Adam', 45, gender='M', job='Engineer') #结果name: Adam age: 45 other: {'gender': 'M', 'job': 'Engineer'} extra = {'city': 'Beijing', 'job': 'Engineer'} person('Jack', 24, **extra) #结果name: Jack age: 24 other: {'city': 'Beijing', 'job': 'Engineer'} ","date":"2022-04-02","objectID":"/python/:2:1","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"2.2函数递归 #类似于C语言 ","date":"2022-04-02","objectID":"/python/:2:2","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"3.高级特性 ","date":"2022-04-02","objectID":"/python/:3:0","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"切片 适用对象：list,tuple 用法 L = ['Michael', 'Sarah', 'Tracy', 'Bob', 'Jack'] L[0:3] = L[:3] = ['Michael', 'Sarah', 'Tracy'] L[-2:] = ['Bob', 'Jack'] L = list(range(100)) L[:10:2] = [0, 2, 4, 6, 8] L[::5] = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95] ","date":"2022-04-02","objectID":"/python/:3:1","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"迭代 \u003e\u003e\u003e d = {'a': 1, 'b': 2, 'c': 3} \u003e\u003e\u003e for key in d: ... print(key) ... a c b #如果要迭代value，可以用for value in d.values()，如果要同时迭代key和value，可以用for k, v in d.items() \u003e\u003e\u003e for i, value in enumerate(['A', 'B', 'C']): ... print(i, value) ... 0 A 1 B 2 C #Python内置的enumerate函数可以把一个list变成索引-元素对 \u003e\u003e\u003e for x, y in [(1, 1), (2, 4), (3, 9)]: ... print(x, y) ... 1 1 2 4 3 9 ","date":"2022-04-02","objectID":"/python/:3:2","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"列表生成式 运用列表生成式，可以快速生成list，可以通过一个list推导出另一个list，而代码却十分简洁。 \u003e\u003e\u003e [x * x for x in range(1, 11)] [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] \u003e\u003e\u003e [x * x for x in range(1, 11) if x % 2 == 0] [4, 16, 36, 64, 100] \u003e\u003e\u003e [m + n for m in 'ABC' for n in 'XYZ'] ['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'] \u003e\u003e\u003e d = {'x': 'A', 'y': 'B', 'z': 'C' } \u003e\u003e\u003e [k + '=' + v for k, v in d.items()] ['y=B', 'x=A', 'z=C'] ","date":"2022-04-02","objectID":"/python/:3:3","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"生成器 用来保存一种算法 获取下一个值使用next(g) #普通用法(注意与生成式区分) \u003e\u003e\u003e g = (x * x for x in range(10)) \u003e\u003e\u003e g \u003cgenerator object \u003cgenexpr\u003e at 0x1022ef630\u003e \u003e\u003e\u003e for n in g: ... print(n) ... 0 1 4 9 16 25 36 49 64 81 #通过函数来构建生成器 #在执行过程中，遇到yield就中断，下次又继续执行。 def fib(max): n, a, b = 0, 0, 1 while n \u003c max: yield b a, b = b, a + b n = n + 1 return 'done' \u003e\u003e\u003e f = fib(6) \u003e\u003e\u003e f \u003cgenerator object fib at 0x104feaaa0\u003e ","date":"2022-04-02","objectID":"/python/:3:4","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"迭代器 凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 可以通过list(Iterrator)此种方式得到一个Interable Python的for循环本质上就是通过不断调用next()函数实现的. for x in [1, 2, 3, 4, 5]: pass 等价于 首先获得Iterator对象: it = iter([1, 2, 3, 4, 5]) # 循环: while True: try: # 获得下一个值: x = next(it) except StopIteration: # 遇到StopIteration就退出循环 break ","date":"2022-04-02","objectID":"/python/:3:5","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"4.函数式编程 特点：允许把函数本身作为参数传入另一函数,还允许返回一个函数 python中变量可以指向函数 python中函数名也是变量 ","date":"2022-04-02","objectID":"/python/:4:0","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"高阶函数 可以把函数作为参数传入的函数称为高阶函数 def add(x,y,f): return f(x) + f(y) #调用 \u003e\u003e\u003e print(add(-5,6,abs)) 11 map(f,list)函数把list中的每个数都按照f的运算规则计算，得出一个Iterator \u003e\u003e\u003e def f(x): ... return x * x ... \u003e\u003e\u003e r = map(f, [1, 2, 3, 4, 5, 6, 7, 8, 9]) \u003e\u003e\u003e list(r) [1, 4, 9, 16, 25, 36, 49, 64, 81] reduce(f,list)函数把list中前两个元素按f计算，再将结果继续和序列的下一个元素进行累计计算 \u003e\u003e\u003e from functools import reduce \u003e\u003e\u003e def add(x, y): ... return x + y ... \u003e\u003e\u003e reduce(add, [1, 3, 5, 7, 9]) 25 filter(f,list)函数函数把list中的每个数都按照f的运算规则计算，如果运算结果是True则保留该元素，否则丢弃该元素。 def is_odd(n): return n % 2 == 1 list(filter(is_odd, [1, 2, 4, 5, 6, 9, 10, 15])) # 结果: [1, 5, 9, 15] sorted()可以对list排序，同时也是高阶参数，可以接收一个参数key来实现自定义排序 key作用于每个数据，然后根据返回的list进行排序 默认情况下，对字符串排序，是按照ASCII的大小比较的 \u003e\u003e\u003e sorted([36, 5, -12, 9, -21]) [-21, -12, 5, 9, 36] \u003e\u003e\u003e sorted([36, 5, -12, 9, -21], key=abs) [5, 9, -12, -21, 36] ","date":"2022-04-02","objectID":"/python/:4:1","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"5.模块 模块需要用import关键字导入 每一个以.py为结尾的文件都是一个python模块 模块中定义的函数和全局变量都是一个模块中可以提供给外界直接使用的工具 import a a.printa() a.name 模块也是一个标识符 标识符由字母、下划线和数字组成 标识符不能以数字开头 不能和关键字重名 模块作用域 类似__xxx__这样的变量是特殊变量，可以被直接引用，但是有特殊用途 似_xxx和__xxx这样的函数或变量就是非公开的（private），不应该被直接引用 ","date":"2022-04-02","objectID":"/python/:5:0","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"模块规范 #!/usr/bin/env python3 # -*- coding: utf-8 -*- ' a test module ' __author__ = 'Michael Liao' import sys def test(): args = sys.argv if len(args)==1: print('Hello, world!') elif len(args)==2: print('Hello, %s!' % args[1]) else: print('Too many arguments!') if __name__=='__main__': test() 第1行和第2行是标准注释，第1行注释可以让这个hello.py文件直接在Unix/Linux/Mac上运行，第2行注释表示.py文件本身使用标准UTF-8编码； 第4行是一个字符串，表示模块的文档注释，任何模块代码的第一个字符串都被视为模块的文档注释； 第6行使用__author__变量把作者写进去 模块是一个包含Python定义和语句的文件。文件名就是模块名后跟文件后缀 .py 。在一个模块内部，模块名（作为一个字符串）可以通过全局变量 __name__ 的值获得。 ","date":"2022-04-02","objectID":"/python/:5:1","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"6.面向对象编程 ","date":"2022-04-02","objectID":"/python/:6:0","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"6.1面向对象与面向过程的区别 面向对象是对面向过程进一步的封装，不仅封装了相同的方法，还将相同属性封装在了一起 ","date":"2022-04-02","objectID":"/python/:6:1","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"6.2类与对象 6.2.1类与对象的概念 6.2.2类的设计 类的命名满足大驼峰原则 属性 方法 参考面向对象编程的概念 ","date":"2022-04-02","objectID":"/python/:6:2","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"6.3面向对象的基础语法 6.3.1dir内置函数 python中，变量，数据，函数都是对象 在python中可用两种方法验证： 在标识符/数据后面输入一个.，按下tab键,会提示方法列表 使用内置函数dir传入标识符/数据,可以查看对象内的所有属性和方法 def demo()，dir(demo) 提示，__方法名__格式的方法是python提供的内置方法/属性 ","date":"2022-04-02","objectID":"/python/:6:3","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["编程语言"],"content":"6.3.2定义简单的类 6.3.2.1定义只包含方法的类 python中语法格式如下 class 类名: def 方法一(self,参数): pass def 方法二(self,参数): pass 方法与函数几乎一样 方法中的特殊处在于,第一个参数必须是self ","date":"2022-04-02","objectID":"/python/:6:4","tags":["编程语言"],"title":"Python学习笔记","uri":"/python/"},{"categories":["RNNs","深度学习","机器学习"],"content":"SRU ","date":"2022-03-28","objectID":"/sru/:0:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SRU","uri":"/sru/"},{"categories":["RNNs","深度学习","机器学习"],"content":"问题和动机 模型的性能依赖于模型的参数和计算量，但RNNs的并行计算能力严重拖累了模型的训练 RNNs的可扩展能力主要受限于RNNs模型的计算序列依赖性，最近的模型为了可扩展性广泛使用的是CNN和注意力机制 因此作者提出了一种平衡了循环依赖和独立性的新模型SRU SRU的状态计算与时间相关，但每个状态都是独立的，可以实现CUDA级优化。 摒弃了卷积的使用，具有更多的循环，保留了建模能力，减少了计算复杂度和超参数。 使用highway连接和精细的参数初始化方案，改进了模型的训练 ","date":"2022-03-28","objectID":"/sru/:1:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SRU","uri":"/sru/"},{"categories":["RNNs","深度学习","机器学习"],"content":"过去的解决方法 QRNN将字符级卷积合并到RNN模型中，加速模型计算 IRNN使用单位对角矩阵初始化RNN的隐藏层的参数，防止梯度消失 [^2]通过将矩阵进行分解从而减少参数量(本质上就是将参数矩阵(m*n)直接写为(m*r)和(r*n)使得，m*r+r*n小于m*n) ","date":"2022-03-28","objectID":"/sru/:2:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SRU","uri":"/sru/"},{"categories":["编程语言"],"content":"C++的学习笔记","date":"2022-03-22","objectID":"/c-/","tags":["编程语言"],"title":"C++学习笔记","uri":"/c-/"},{"categories":["编程语言"],"content":"C++学习笔记 ","date":"2022-03-22","objectID":"/c-/:0:0","tags":["编程语言"],"title":"C++学习笔记","uri":"/c-/"},{"categories":["编程语言"],"content":"C++简单特性 编译型语言，不是解释性 源代码的可移植性，但可执行程序不跨平台 由C++核心语言特征以及标准库两个组件构成 核心语言特性例如内建类型和循环 标准库是由各个C++实现提供源代码，能够由C++本身实现（自举），例如容器或者I/O库等 静态类型语言 ","date":"2022-03-22","objectID":"/c-/:1:0","tags":["编程语言"],"title":"C++学习笔记","uri":"/c-/"},{"categories":["编程语言"],"content":"函数 函数必须要提前声明 函数的参数名字会被编译器忽略 编译时会发生参数类型检查或者隐形变量类型转换 类的成员函数，类名也是成员函数类型的一部分 两个函数具有同样的名字（函数重载） 但具有不同的参数，编译器会自动选择最合适的。 两者在调用时没有过分的区别，会报错ambiguous 基于可维护性的原则，应该让程序可理解，首先需要将任务分解为函数和类的模块 强迫我们声明各个类或者函数的功能和依赖关系。 便于复用。 便于在大型程序中定位Bug。 ","date":"2022-03-22","objectID":"/c-/:1:1","tags":["编程语言"],"title":"C++学习笔记","uri":"/c-/"},{"categories":["编程语言"],"content":"类型变量和基本运算符 声明将实体引入到程序中 类型定义了一组可能的值和一组可能的操作 对象是保存着某种类型的值的内存空间 值是根据类型解释的一组比特 变量是命名了的对象 类型决定了变量存储的值的范围 基本类型和硬件设备关联 基本类型不同的系统可以有不同的大小实现 类型的大小可以通过sizeof()操作来获得 ","date":"2022-03-22","objectID":"/c-/:1:2","tags":["编程语言"],"title":"C++学习笔记","uri":"/c-/"},{"categories":["RNNs","深度学习","机器学习"],"content":"QRNN1 ","date":"2022-02-19","objectID":"/qrnn/:0:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Novelty 将RNN与CNN结合 卷积层用一种不使用未来数据的masked卷积来代替LSTM中参数矩阵与上一时刻隐藏层相乘的操作。捕获过去时刻信息的同时简化了LSTM中的的操作，因为在计算过程中删除了隐藏层，仅仅使用输入数据来捕获依赖 池化层使用各种门结构，例如LSTM的门结构和GRU的门结构使得梯度流稳定 通过上述操作简化了LSTM的计算，使其可以并行 ","date":"2022-02-19","objectID":"/qrnn/:1:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"问题和动机 标准的RNN包括门变种LSTM等因为无法并行计算，因此在长序列的任务中性能受到了限制。 将CNN用于序列模型时 并行性更好 可以更好地扩展到长序列 但因为最大和平均池化时假设了时间不变性，（在一次卷积池化过程中，时间步的顺序会被忽略，移动卷积核的过程中，进行相同的池化操作，不同时间步的重要性不同同样也会被忽略）因此无法充分利用大规模序列的顺序信息。 因此作者提出了一种将CNN和RNN混合的模型QRNN，既能跨时间步和小批量维度进行并行计算，又使得输出取决于总体顺序。性能更优秀且更节省时间 ","date":"2022-02-19","objectID":"/qrnn/:2:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"过去的解决方法 将CNN应用到序列模型2character-level CNN(NIPS2015)假设了时间不变性，无法利用顺序信息 该模型主要通过多个卷积层和池化层堆叠，沿时间步对序列进行卷积和池化来实现序列信息的捕获 音频生成领域的waveNet模型3，通过五层类似于CNN的结构取得了很好效果。但这一模型在音频生成这种上一个时刻依赖很强烈的领域可能有效，但是对于NLP的其他领域并没有取得很好的效果。 使用了因果卷积的假设，假设了当前时间仅仅依赖上一个时刻，但因果卷积的感受野较小，通常需要多层或者大型过滤器来增加感受野 因果卷积的模型图如上所示，因果卷积的感受野较小 通过扩展卷积将感受野增大几个数量级，同时没有增加计算成本 扩展卷积的模型图如上所示，通过简单的方式增大了CNN的感受野 将CNN与RNN结合4(待补充) ","date":"2022-02-19","objectID":"/qrnn/:3:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"模型 QRNN模型由两个组件构成，类似于CNN模型中的卷积层和池化层。卷积层允许在Minibatch和空间维度（顺序序列）两个层面并行化训练。池化层允许在Minibatch和特征维度两个层面并行化训练，并且池化层没有可训练的参数。 ","date":"2022-02-19","objectID":"/qrnn/:4:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"卷积组件 为了确保对于下一个任务的预测能力，卷积操作时不能使用未来的数据，因此使用一种masked convolution。 假设输入序列为$X \\in \\mathbb{R}^{T \\times n}$，T为时间序列的长度，n为每个输入向量$x$的维度。$Z \\in \\mathbb{R}^{T \\times m}$为卷积操作之后隐藏层序列，其中m为每个输入对应隐藏层的维度。卷积操作如下所示 \\begin{align*} Z \u0026= tanh(W_z * X) \\\\ F \u0026= \\sigma(W_f * X) \\\\ O \u0026= \\sigma(W_o * X) \\end{align*} 其中$W_z,W_f,W_o \\in \\mathbb{R}^{k \\times n \\times m}$，k为卷积过滤器的长度，$ * $ 代表卷积操作，例如k取2时，对于每个元素而言，上述公式可写为 \\begin{align*} z_t \u0026= tanh(W^1_z x_{t-1} + W^2_z x_{t}) \\\\ f_t \u0026= \\sigma(W^1_f x_{t-1} + W^2_f x_{t}) \\\\ o_t \u0026= \\sigma(W^1_o x_{t-1} + W^2_o x_{t}) \\end{align*} 对比LSTM中的运算 可以看出去掉了隐藏层$H_t$，$z_t$代替了$\\tilde{C}$，$f_t$代替了$F_t$，$o_t$代替了$O_t$ \\begin{align*} I_t \u0026= \\sigma(X_tW_{xi}+H_{t-1}W_{hi}+b_i) \\\\ F_t \u0026= \\sigma(X_tW_{xf}+H_{t-1}W_{hf}+b_f) \\\\ O_t \u0026= \\sigma(X_tW_{xo}+H_{t-1}W_{ho}+b_o) \\\\ \\tilde{C}_t \u0026= tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c) \\end{align*} ","date":"2022-02-19","objectID":"/qrnn/:4:1","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"池化组件 对于池化组件而言，有多种选择，第一种选择仅仅使用遗忘门，第二种选择使用遗忘门和输出门，第三种选择使用遗忘门，输入们和输出门。如下所示 仅使用遗忘门 $$ h_t = f_t \\odot h_{t-1} + (1 - f_t) \\odot z_t $$ 使用遗忘门和输出门 \\begin{align*} c_t \u0026= f_t \\odot c_{t-1} + (1 - f_t) \\odot z_t \\\\ h_t \u0026= o_t \\odot c_t \\end{align*} 使用遗忘门，输入门和输出门 \\begin{align*} c_t \u0026= f_t \\odot c_{t-1} + i_t \\odot z_t \\\\ h_t \u0026= o_t \\odot c_t \\end{align*} 与之相比，LSTM中的最后一步计算如下所示 \\begin{align*} C_t \u0026= F_t \\odot C_{t-1} + I_t \\odot \\tilde{C}_t \\\\ H_t \u0026= O_t \\odot tanh(C_t) \\end{align*} ","date":"2022-02-19","objectID":"/qrnn/:4:2","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"变种 对于不同的任务而言可以对原始的QRNN模型进行各种改进。由于QRNN结合了CNN和RNN模型。因此对CNN和RNN模型的改进也可以应用到QRNN上。 正则化(对于RNN的改进) 传统的dropout方法难以在循环连接结构上产生高效的结果 基于变分推理的正则化(variational inference–based dropout)5，这种方式不适合于QRNN zoneout6 DenseNet7(对于CNN的改进) denseNet是一种在CNN模型中较为有效的连接结构,通过将层与层之间的连接二次方化，改善了梯度流和收敛性，论文的作者发现，将此连接方式用于QRNN的序列分类任务时，可以取得一定的效果提升 编码解码模型 由于传统的编码解码器结构，只是将编码器的最后一层作为解码器的第一个池化层的输入，并没有影响到解码器的卷积层，限制了模型的表现能力。 作者想到将编码器的最后一层状态合并到解码器的每一个时间步的卷积操作中，可以取得较好的效果。模型被修改为以下结构,其中H为编码器的最后一层隐藏层状态 我个人觉得这是实验效果不错，所以写上去的理由，引入的CNN结构或者门结构都可以捕获时序信息，只要第一个被输入到卷积层或者池化层了，后面的时间步也能获取到对应的信息。这样的改进结构类似于只有编码器对后面的层使用DenseNet连接，其实是改进了卷积层的CNN操作 \\begin{align*} Z\u0026=tanh(W_z * X + V_z * H) \\\\ F\u0026=\\sigma(W_f * X + V_f * H) \\\\ O\u0026=\\sigma(W_o * X + V_o * H) \\end{align*} 除此之外，根据论文8的结果，将一个softmax卷积层放到输出前可以提升模型效果，因为不加卷积层时，仅仅是通过固定长度的向量来对所有输入进行表示。 在解码时引入注意力机制，具体如下 通过将编码器的最后一层的每一个时间步的隐藏层状态与时间步t的门操作之前的解码器状态进行内积，然后用softmax操作，得到蕴含所有时间步的编码器隐藏状态的变量$\\alpha_{at}$，然后通过此变量得到注意力总和$k_t$，最后通过门操作得到$h_t^L$，卷积操作如下所示 $$ \\begin{aligned} \\alpha_{st}\u0026=\\mathop{softmax}\\limits_{all\\,s}(c_t^L \\cdot \\tilde{h}_s^L) \\\\ k_t\u0026=\\sum_s\\alpha_{st}\\tilde{h}_s^L \\\\ h_t^L \u0026= o_t \\odot(W_kk_t+W_cc_t^L) \\end{aligned} $$ QRNN使用编码器解码器结构来改进模型 ","date":"2022-02-19","objectID":"/qrnn/:4:3","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"实验 本论文在三种任务上评判了模型的性能。分别是：文档级别的情绪分类，语言模型和基于字符的神经机器翻译。 QUASI-RECURRENT NEURAL NETWORKS ↩︎ Character-level Convolutional Networks for Text Classification(NIPS2015) ↩︎ WaveNet: A Generative Model for Raw Audio ↩︎ Fully Character-Level Neural Machine Translation without Explicit Segmentation ↩︎ Yarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural networks. In NIPS, 2016. ↩︎ David Krueger, Tegan Maharaj, János Kramár, Mohammad Pezeshki, Nicolas Ballas, Nan Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron Courville, et al. Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations. ↩︎ Densely connected convolution networks.(CVPR2017) ↩︎ Neural machine translation by jointly learning to align and translate(ICLR 2015) ↩︎ ","date":"2022-02-19","objectID":"/qrnn/:5:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"QRNN","uri":"/qrnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"SCRN模型1 ","date":"2022-02-17","objectID":"/scrn/:0:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Novelty 通过为RNN添加一个参数缓慢变化的隐藏层来捕获长距离依赖，公式如下所示 \\begin{align*} s_t \u0026= (1-\\alpha)Bx_t + \\alpha s_{t-1} \\\\ h_t \u0026= \\sigma(Ps_t+Ax_t+Rh_{t-1}) \\\\ y_t \u0026= f(Uh_t+Vs_t) \\end{align*} 约束该隐藏层中的B为对角矩阵，由于B为对角矩阵，且没有使用激活函数，所以该隐藏层的梯度流稳定 RNN和SCRN的架构图 ","date":"2022-02-17","objectID":"/scrn/:1:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"问题和动机 前馈神经网络中的时滞神经网络2，通过一个固定长度的最近历史窗口的结构，达到了能够建模序列数据的目的。但也有以下缺点 固定长度的窗口难以学习到长距离依赖 只能接受线性增长的参数代价 简单的递归神经网络(SRN)由于梯度消失问题同样难以学习到长距离依赖 部分非线性激活函数，例如sigmoid使得任何地方的梯度都接近零。(可以通过使用ReLu等激活函数来部分解决这个问题) BPTT算法出现矩阵连乘现象，如果矩阵特征值很小(小于1)，梯度会迅速收敛到0。 过去的带有语境特征(contextual feature)的SRU使用NLP的预训练技术将其引入，并没有将其作为循环网络的一部分来训练 本文提出一种较为简单，将语境特征作为模型的一部分来训练的模型 ","date":"2022-02-17","objectID":"/scrn/:2:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"过去的解决方法 用Hessian-Free来代替SGD LSTM模型 用语境特征(contextual feature)进行预训练来引入长距离的上下文信息 ","date":"2022-02-17","objectID":"/scrn/:3:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Idea (trick)使用分层softmax函数来代替原本的softmax函数，因为计算softmax函数的normalization项通常是性能瓶颈(会损失表现性能) 使用梯度重归一来避免梯度爆炸(相当于梯度裁剪) 非线性激活会导致梯度消失，完全连接的隐藏层在每个时间步都会完全改变状态(参数矩阵完全改变)，因此增加一个参数矩阵为单位矩阵，且没有非线性激活的隐藏层。如下式所示 $$s_t = s_{t-1}+Bx_t$$ 最后的模型如下所示 $$ \\begin{aligned} s_t \u0026= (1-\\alpha)Bx_t + \\alpha s_{t-1} \\\\ h_t \u0026= \\sigma(Ps_t+Ax_t+Rh_{t-1}) \\\\ y_t \u0026= f(Uh_t+Vs_t) \\end{aligned} $$ 上述公式固定了$\\alpha$，因此只能学习固定的时间尺度。如果将其设为可训练的话，那么就能从不同的时间延迟上学习。如下所示 $$ \\begin{aligned} s_t \u0026= (I-Q)Bx_t + Q s_{t-1} \\\\ h_t \u0026= \\sigma(Ps_t+Ax_t+Rh_{t-1}) \\\\ y_t \u0026= f(Uh_t+Vs_t) \\\\ diag(Q) \u0026= \\sigma(\\beta) \\end{aligned} $$ * 其中Q是对角矩阵，约束对角线元素为对参数向量β做sigmoid变换。这种约束可以使得对角权重保持在0和1之间 论文作者提到，只要还使用标准隐藏层，固定$\\alpha$与将其作为可训练的参数区别不大。 ","date":"2022-02-17","objectID":"/scrn/:4:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"实验 实现中将$\\alpha$固定为0.95，BPTT的步数为50，普通SRN步数为10。每向前进行5步就进行一次梯度下降，batchsize设为32，初始学习率为0.05，当验证误差不再减少时，每次训练完成后，将学习率除以1.5。 ","date":"2022-02-17","objectID":"/scrn/:5:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Penn Treebank Corpus数据集 固定$\\alpha$参数 在参数较少的小数据集上，SCRN和LSTM具有相当的性能，当LSTM具有更多的模型参数，大约是4倍。与\"leaky neurons\"相比，SCRN也有较大改善。结果如下图所示 固定参数 学习自适应权重 当隐藏层的参数大小很小时，学习自适应权重是有效的，但当参数规模逐渐变大的时候，并没有带来显著的改善。结果如下图所示 学习自适应权重 ","date":"2022-02-17","objectID":"/scrn/:5:1","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Text8数据集 SCRN随着隐藏层和context参数的增加，性能会逐步提升。如下图所示 当参数规模较小时，SCRN优于LSTM，但参数规模变大之后还是LSTM的性能更好，如下图所示 SRN，LSTM与SCRN的对比 ","date":"2022-02-17","objectID":"/scrn/:5:2","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"结论 当参数规模受到限制时，SCRN会大大优于LSTM。但当参数规模变大时，有相似的性能。这些所有的模型都不能真正学习到长时记忆。本文还在github上开源了代码3 ","date":"2022-02-17","objectID":"/scrn/:6:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"思考中的问题 单位矩阵在模型中起到的作用(推导反向传播) Learning lo nger memory in recurrent neural networks. ↩︎ Rumelhart, David E, Hinton, Geoffrey E, and Williams, Ronald J. Learning internal representations by error propagation. Technical report, DTIC Document, 1985. ↩︎ http://github.com/facebook/SCRNNs  ↩︎ ","date":"2022-02-17","objectID":"/scrn/:7:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"SCRN","uri":"/scrn/"},{"categories":["机器学习"],"content":"矩阵求导 ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:0:0","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"分子布局和分母布局 标量，向量，矩阵之间的求导，相对于标量对标量的求导，需要考虑一个额外的因素，就是求导之后的布局，例如标量对列向量求导之后是按照列向量排列还是按照行向量排列并没有一个确切的规定。因此这里我们引入分子布局和分母布局的概念。 分子布局指的是求导之后的排列方式和维度以分子为主 例如对于列向量$\\mathbf{y}$而言，$\\frac{\\partial \\mathbf{y}}{x}$，按照分子布局，得到的结果也是列向量。 一个标量$y$对 $m\\times n$维的矩阵 $X$求导，其结果为$n\\times m$维。 一个$m$维的列向量$\\mathbf{y}$对一个$n$维的行向量$\\mathbf{x}$求导，其结果为 $$ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}= \\begin{Bmatrix} \\frac{\\partial y_1}{\\partial x_1} \u0026 \\frac{\\partial y_1}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_1}{\\partial x_n}\\\\ \\frac{\\partial y_2}{\\partial x_1} \u0026 \\frac{\\partial y_2}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_2}{\\partial x_n}\\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1} \u0026 \\frac{\\partial y_m}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_n}\\\\ \\end{Bmatrix} $$ 是一个$m \\times n$维的矩阵，也被叫做雅可比矩阵 分母布局指的是求导之后的排列方式和维度以分母为主 对于列向量$\\mathbf{y}$而言，$\\frac{\\partial \\mathbf{y}}{x}$，按照分母布局，得到的结果是行向量 一个标量$y$对 $m\\times n$维的矩阵$X$求导，其结果为$m\\times n$维。 一个$m$维的列向量$\\mathbf{y}$对一个$n$维的行向量$\\mathbf{x}$求导，其结果为 $$ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}= \\begin{Bmatrix} \\frac{\\partial y_1}{\\partial x_1} \u0026 \\frac{\\partial y_2}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_1}\\\\ \\frac{\\partial y_1}{\\partial x_2} \u0026 \\frac{\\partial y_2}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_2}\\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial y_1}{\\partial x_n} \u0026 \\frac{\\partial y_2}{\\partial x_n} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_n}\\\\ \\end{Bmatrix} $$ 是一个$n \\times m$维的矩阵，也被叫做梯度矩阵 因此对于标量，向量和矩阵的求导可按下述表格来定义： 自变量/因变量 标量$y$ $m$维列向量$\\mathbf{y}$ $m \\times n$维矩阵$Y$ 标量$x$ / $\\frac{\\partial \\mathbf{y}}{\\partial x}$ 分子布局：$m$维列向量 分母布局：$m$维行向量 分子布局：$m \\times n$维矩阵 分母布局：$n \\times m$维矩阵 $m$维列向量$\\mathbf{x}$ $\\frac{\\partial y}{\\partial \\mathbf{x}}$ 分子布局：$m$维行向量 分母布局：$m$维列向量 $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ 分子布局：$m \\times n$维矩阵 分母布局：$n \\times m$维矩阵 / $m \\times n$维矩阵$X$ $\\frac{\\partial y}{X}$ 分子布局：$n \\times m$维矩阵 分母布局：$m \\times n$维矩阵 / / ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:1:0","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"求导方法–微分法 本节讨论的内容基于分母布局 ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:2:0","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"标量和向量的微分和导数 在微积分中对于标量的导数和微分有如下公式 $$ df=f^\\prime dx $$ 多变量的情况下，则有如下公式（其中$\\mathbf{x}$为列向量）: $$df=\\sum\\limits_{i=1}^n\\frac{\\partial f}{\\partial x_i}dx_i = (\\frac{\\partial f}{\\partial \\mathbf{x}})^Td\\mathbf{x}$$ 多变量的情况其实也是标量对向量的微分 ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:2:1","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"矩阵的微分和导数 上述标量的微分和导数同样可以扩展到矩阵上，对于矩阵微分，有如下定义（其中$X$为矩阵，$X_{i,j}$为矩阵中位置为$i,j$的值）: $$df = \\sum\\limits_{i=1}^m \\sum\\limits_{j=1}^n\\frac{\\partial f}{\\partial X_{i,j}}dX_{i,j} = tr((\\frac{\\partial f}{\\partial X})^TdX)$$ 因为矩阵的乘法是前一个矩阵的行和后一个矩阵的列逐项相乘，对偏导加转置，便于我们在最后矩阵的基础上找到一组较为方便的元素来得到最后的全微分。 同时我们只需要对应的偏导数和对应的元素的微分的乘积，所以最后取最后结果的对角线元素的和。 有了上述的规则，我们仍然很难将实际问题中的导数求解出来，因此需要引入一些额外的性质来方便计算。 迹的性质 标量的迹等于自己：$tr(x) = x$ 转置不变：$tr(A^T)=tr(A)$ 交换律：$tr(AB)=tr(BA)$，需要A,B同维度 加减法：$tr(A \\pm B) = tr(A) \\pm tr(B)$ 矩阵的乘法和转置：$tr((A\\odot B)^TC) = tr(A^T(B\\odot C ))$，需要A,B,C同维度 矩阵微分的性质 加减法：$d(X\\pm Y) = d(X) \\pm d(Y)$ 乘法：$d(XY)=d(X)Y + Xd(Y)$ 对元素重新排列：$d(X^\\ast) = d(X)^\\ast$例如转置等 迹：$dtr(X)=tr(dX)$ 哈德曼积：$d(X \\odot Y) = X \\odot dY + dX \\odot Y$ 逐元素求导：$d\\sigma(X) = \\sigma^\\prime(X) \\odot dX$ 逆：$dX^{-1}=-X^{-1}(dX)X^{-1}$ 行列式：$d |X|= |X|tr(X^{-1}dX),d (ln|X|)= tr(X^{-1}dX)$ 计算的原则是给标量的微分套上迹进行计算，具体计算实例可参考知乎专栏矩阵求导术1 https://zhuanlan.zhihu.com/p/24709748  ↩︎ ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:2:2","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"机器学习中的分布式的并行优化 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:0:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"简介 机器学习的并行计算指的是对机器学习过程中遇到的计算问题采用并行计算的方式进行合适的加速，缩短训练所需的时间。此处所指的时间既指物理时间，也指CPU时间或者GPU时间。 和其他大数据计算问题类似，有两种途径来实现对计算的并行加速，一种是向单一机器添加更多计算资源，可以称之为纵向扩展，另外一种是类似分布式系统一样，在系统中添加更多的节点，节点可能是CPU，GPU甚至是单机环境等等，可以称之为横向扩展。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:1:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"纵向扩展 CPU或者GPU时间的缩短主要应用纵向扩展，通过对特定问题设计特定的CPU或者GPU模块及指令集等，通过硬件加速机器学习算法中的特定问题。虽然机器学习近年来涌现出各种各样互不相同的算法，但是对于底层的计算而言，这些算法所使用的数据操作的本质基本相似，都是线性代数中的基本操作，是对向量，矩阵，张量的基本计算，因此涌现出了很多种加速方法和实现方案。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:2:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"主要策略 纵向扩展中的常用方法是，添加可编程GPU。最初，GPU用于机器学习的应用受到限制，因为GPU使用纯SIMD（单指令多数据）模型，不允许内核执行不同的代码分支；所有线程都必须执行完全相同的程序。 后来出现的通用GPU，即可以执行任意代码的GPU。这些产品可以作为加速器添加到传统机器上，加快了机器学习的训练效率。例如Nvidia的Titan V和Tesla V100显卡就可以显著加速机器学习的学习和训练。 除了使用通用GPU加速之外，还可以使用专用集成电路（ASIC）来加速机器学习，专用集成电路主要通过高度优化的设计实现特定功能。 在最近几代产品中，即使是通用CPU也增加了向量指令的可用性和宽度，以加速计算密集型问题（如机器学习算法）的处理。这些指令是矢量指令，是AVX-512系列的一部分，具有增强的字变量精度并支持单精度浮点运算。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:2:1","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"横向扩展 采用纵向扩展的优化方法之外，还可以对单一问题进行拆解，使其可以在多处理器，多显卡，多机的环境下进行计算，主要解决单处理器，单显卡，单机性能提升较慢，难以跟上深度学习中计算资源的需求问题，通过数量来加速计算。在工程实践中采用横向扩展的案例也并不少见。 采用横向扩展，单机设备的成本较低，同时易于增加算力。 单机设备在计算过程中发生故障时往往难以恢复，对于多机环境而言，单个处理器发生故障时，系统仍然可以通过启动部分恢复（例如，基于通信驱动的检查点1或部分重新计算2）继续运行。 单处理器环境下，对于大规模数据的读取可能有IO瓶颈。 横向扩展的一个主要挑战是，并非所有ML算法都适用于分布式计算模型。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:3:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"数据并行和模型并行 数据并行将原始数据分配到不同的工作节点上并行训练。其中，每一个工作节点使用不同的部分数据，但是都拥有完整的模型，工作节点之间一般会同步自己的局部梯度信息，再进行汇总，得到整体的更新结果。数据并行依赖于优化算法的选择。 模型并行一般是由于模型太大，单机无法储存，将模型的不同部分放在不同的节点上进行训练，常用的方式是每一个节点均使用相同的数据，但是只使用模型的一部分来进行。因此模型并行依赖于模型的设计。 在实际应用过程中，数据并行更为常见。 模型并行和数据并行 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:3:1","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"主要策略 横向扩展组成的分布式机器学习系统中，性能主要取决于三部分，计算瓶颈，IO瓶颈和通信瓶颈。 由于IO和通信的限制，虽然机器数量在不断增加，但加速效果并不是线性提升，在增加数量过程中，会产生性能损耗，比如，增加了十倍的机器，理想训练速度能够增加十倍，实际上往往却只增加了一两倍，性价比很低。这实际上是因为IO和通信瓶颈所导致的。 期望能够提高加速比，使得分布式机器学习可以更快，就需要降低通信和IO时间开销的同时，加快计算性能，才能提升计算的时间占比，使得性能损耗更小。 下面从这三个点来展开横向扩展的优化策略。 通信优化 通信上，一方面提升通信速度，比如通信拓扑的改进，通信步调和频率的优化，另一方面也可以减少通信内容和次数，比如梯度压缩和梯度融合技术等。 通信方式 Share memory：同的处理器共享一块内存，没办法同时用很多处理器进行工作 Message passing：有多个节点，节点的处理器之间是可以采用共享内存，节点之间不能共享内存。节点之间可以网线相连接也可以使用 TCP/IP 进行消息传递。需要注意的是，采用Message passing方法时通常使用MPI的标准库来进行并行通信。根据节点的协调方式可以分为两类 点对点(point-to-point)通信，这是高性能计算(HPC)中最常使用的模式，通常是与其最近的邻居进行通信，每个实例都是单发送方，单接收方 集合(collective)通信，也可以叫做C/S架构，存在多个发送方和接收方。 通信步调 Bulk Synchronous Parallel（BSP） 是最简单的模型，其中程序通过同步每个计算和通信阶段来确保一致性。遵循BSP模型的程序示例是MapReduce。优点是ML程序可以保证输出正确的解决方案。缺点是，完成的节点必须在每个同步障碍处等待，直到所有节点完成，这会导致在某些节点进度比其他工作人员慢的情况下产生开销。 Stale Synchronous Parallel（SSP）通过允许速度更快的节点向前移动一定数量的迭代来缓解同步开销。如果超过此数字，则暂停所有工作进程。节点在缓存的数据版本上操作，并且仅在任务周期结束时提交更改，这可能会导致其他节点在过时的数据版本上操作。数据SSP的主要优点是它仍然享有强大的模型收敛保证。然而，缺点是，当陈旧性变得太高时（例如，当大量机器减速时），收敛速度会迅速恶化。 Approximate Synchronous Parallel（ASP）限制了参数的不准确程度。这与SSP形成对比，SSP限制了参数的过时程度。一个优点是，每当聚合的更新无关紧要时，服务器都可以延迟同步。一个缺点是，很难选择定义哪些更新重要，哪些更新不重要的参数。 Barrierless Asynchronous Parallel /Total Asynchronous Parallel (BAP/TAP) 让节点并行通信，而无需彼此等待。其优点是通常可以获得尽可能高的加速比。一个缺点是，模型可能收敛缓慢，甚至发展不正确，因为与BSP和SSP不同，误差随延迟而增长。 MapReduce和Spark MapReduce是C/S架构，Server可以把信息广播到worker节点。Server先定义一个 Map 操作，这个 Map 操作是由worker节点完成，然后worker把结果传回client并处理，这个叫做reduce。梯度下降可以用 MapReduce 进行并行化。并行化的过程中，数据被分给 worker 进行计算。每一个梯度下降过程包含一个广播、map和一个 reduce 操作。 MapReduce的主要问题有两个，一是原语的语义过于低级，直接使用其来写复杂算法，开发量比较大；另一个问题是依赖于磁盘进行数据传递，性能跟不上业务需求。 为了解决MapReduce的两个问题，Matei在3中提出了一种新的数据结构RDD，并构建了Spark框架。Spark框架在MR语义之上封装了DAG调度器，极大降低了算法使用的门槛。 Spark是基于内存进行数据处理的，MapReduce是基于磁盘进行数据处理的。 DAG计算模型在迭代计算上还是比MapReduce的效率更高。 MapReduce中，reduce任务需要等待所有map任务完成后才可以开始；在Spark中，分区相同的转换构成流水线放到同一个任务中运行。 较长时间内spark几乎可以说是大规模机器学习的代表，直至后来李沐完善了参数服务器，开拓了大规模机器学习的领域以后，spark才暴露出一点点不足。 参数服务器 参数服务器的概念最早大概可以追溯到Alex Smola于2010年提出的并行LDA的框架，其采用一个分布式的Memcached作为存放参数的存储，用于在分布式系统不同的Worker节点之间同步模型参数，而每个Worker只需要保存它计算时所依赖的一小部分参数。 在此之后，PS又有了很多改进，其中又以李沐2014年提出的ps-lite4(所谓第三代PS架构)为主要代表，也进一步加快了业界广泛使用参数服务器的步伐，在广告，推荐等各领域内大放异彩，时至今日，依然在各大公司内发挥着重要作用。 ps-lite的主要架构示意图如下图所示。 ps-lite架构 其中，resource manager用来对当前的各个计算资源进行管理，可以直接利用资源管理组件如yarn、mesos或者k8s来实现，而底下的training data就是用来采集训练数据，在大规模场景下，一般需要类似GFS的分布式文件系统的支持，剩下的server group和worker group部分就是参数服务器的核心组件了。 Paraeter Server框架中，每个server都只负责分到的部分参数（server共同维持一个全局共享参数）。server节点可以和其他server节点通信，每个server负责自己分到的参数，server group 共同维持所有参数的更新。server manage node负责维护一些元数据的一致性，例如各个节点的状态，参数的分配情况。worker节点之间没有通信，只和对应的server有通信。 每个worker group有一个task scheduler，负责向worker分配任务。一个具体任务运行的时候，task schedule负责通知每个worker加载自己对应的数据，然后去server node上拉取一个要更新的参数分片，用本地数据样本计算参数分片对应的变化量，然后同步给server node；server node在收到本机负责的参数分片对应的所有worker的更新后，对参数分片做一次update。 从通信视角上看，其是一种比较朴素直观的算法过程，可以看成是reduce+broadcast的过程，先是将worker上的信息reduce到server节点上，之后server节点汇总了信息后，再broadcast到worker节点上去，完成了一次信息的处理过程，如下图所示。在这个结构中也能看到，worker之间不通信，而全部依赖于server节点，worker之间的通信能力未得到充分利用， 并且是单工通信，没有同时利用上行带宽和下行带宽，当参数非常稠密，需要通信的信息比较多时，server节点有可能成为瓶颈。 但是如果参数是高维稀疏，单机无法保存全部参数，且每个worker无需访问全部的参数的情况，如推荐中的百亿级feature的LR，LDA，小数据量的通信延迟较低，加上PS架构支持异步更新，可以减少阻塞，加快训练速度。粗略地说，原始的PS架构更适合稀疏超大模型，且更容易容灾，也因此在推荐领域内广泛应用。 ps-lite运行过程 Ring All-Reduce PS架构虽然在很多领域内大放异彩，应用广泛，但是当模型稠密，需要大量交换信息的情况下，Server节点很容易成为瓶颈，限制了其作用，也因此有了将Ring AllReduce这一类通信方法应用到机器学习领域的尝试。 实际上，Ring AllReduce算法在高性能计算领域中已经有了比较长的历史，OpenMPI中至少在2007年就有了关于其的开源实现。然而机器学习领域内的对此知之甚少，更加不知道怎么利用其来加速分布式机器学习的速度。直到2016年，百度的研究人员首次尝试将Ring AllReduce算法应用到深度学习领域内，并在很多问题上取得了明显比PS架构更显著的加速效果，在深度学习领域取得了广泛的关注。 正如名字中所表达，**Ring AllReduce算法首先需要将集群内各个节点按照环状的形式排列，在这个环中，每一个节点都只接收其左邻居节点的信息，且都只发送信息给自己的右邻居节点。**在具体的通信内容和方式的组织上，大概可以分为两部分，第一部分，对于N个节点的集群，将每个节点上数据切分为N份，然后经过N-1轮的Reduce-Scatter过程。具体地，每一轮中，每个GPU将自己的一个chunk发给右邻居，并接收左邻居发来的chunk，并累加，经过这样的步骤，每一个节点都拥有一部分数据的最终结果。第二部分，与上部分相类似，进行N-1轮的AllGather过程，将每一个节点上的一部分的完整信息传递到所有节点上，经过此步骤，每一个节点上就拥有了所有数据的完整信息。 减少通信内容–梯度压缩 梯度压缩里有两大类主要的方案，一是梯度量化的方法，二是梯度稀疏化的方法。 梯度量化 模型量化等技术在模型推理上发展的相对成熟，也已经有很多成功的应用，可以有效的减少模型尺寸，降低模型推理成本。然而，在训练中，目前还不能做到直接用很小的或是量化的模型进行训练，其往往会导致训练的效","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:3:2","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["RNNs","深度学习","机器学习"],"content":"RNN和LSTM(待完成) ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:0:0","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"RNN ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:0","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"问题和动机 自然界中很多事物都是序列相关的，想要理解某一时刻，必须要获取过去时刻的信息 过去的模型难以捕获序列信息 ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:1","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"idea 通过一个状态变量来存储过去的状态 每一时刻的状态都由当前状态和上一时刻的状态共同决定 采用参数共享的思想，每一时刻的参数都是相同的，简化了模型中的参数，简化了运算。 可以与CNN进行对比，CNN同样采用了参数共享(权值共享)的思想 CNN中的参数共享是空间上的，其假设图像的底层特征与图像中的位置无关。(但是后来发现高级特征其实与位置同样有关系，需要用局部全连接层和全连接层混合来建模) RNN中的参数共享是时间上的，每一个时间步的参数都相同 ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:2","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"模型的数学表达 $$ H_t = \\sigma(W_HH_{t-1}+W{_X}X{_t}+b_h)$$ $$O_t = W_OH_t+b_q$$ 这里的$\\sigma$指的是非线性激活函数，在最原始的论文里用的是$Tanh$激活函数(后续改进中也有使用ReLU)，$W_H,W_X,W_O$分别是模型中的三个参数。 ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:3","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"梯度消失和梯度爆炸 RNN模型中一直令人诟病的两个问题是梯度消失和梯度爆炸问题，这两个问题的出现严重阻碍了模型的优化。下面通过随时间的反向传播(BPTT)进行公示的推导来演示梯度消失和梯度爆炸产生的原因。 符号定义 在进行推导之间，我们首先定义如下符号 符号 解释 $K$ 输入的向量维度 $T$ 输入元素的长度 $H$ 隐藏层单元数 $X= \\{ x_1,x_2,…,x_T \\} $ 输入元素的按时间表示表示 $x_t \\in \\mathbb{R}^{K \\times 1}$ $t$时刻RNN的输入 $h_t \\in \\mathbb{R}^{H \\times 1}$ $t$时刻，隐藏层的输出 $o_t \\in \\mathbb{R}^{H \\times 1}$ $t$时刻，模型的输出 $W_H \\in \\mathbb{R}^{H \\times H}$ 隐藏状态的权重参数 $W_X \\in \\mathbb{R}^{H \\times K}$ 输入的权重参数 $W_O \\in \\mathbb{R}^{K \\times H}$ 输出的权重参数 $L_t$ $t$时刻的损失函数的值 $L$ 所有时刻损失函数的值的总和 需要做说明的是上述符号定义并没有就具体问题进行分析，如果将其应用在不同问题上，可能有不同的符号定义方式，为了简单起见，本文仅以该设定为例，来讨论梯度消失和爆炸问题，其他问题均可采用该思想分析。 上述符号在RNN模型公式中的对应关系如下 $$\\begin{cases} h_t = tanh(W_H h_{t-1} + W_X x_{t}) \\\\ o_t = W_O h_t \\\\ L_t = f(o_t,y) \\\\ L = \\sum\\limits_{t=1}^T{L_t} \\end{cases}$$ 反向传播 在随时间反向传播方法中，我们需要求三个偏导数，$\\frac{\\partial L}{\\partial W_H},\\frac{\\partial L}{\\partial W_X},\\frac{\\partial L}{\\partial W_O}$。 1.计算$\\frac{\\partial L_t}{\\partial W_O}$ $$\\begin{aligned} \\frac{\\partial L_t}{\\partial W_O} = \\frac{\\partial L}{\\partial f(o_t,y)} \\cdot \\frac{\\partial f(o_t,y)}{\\partial o_t} \\cdot \\frac{\\partial o_t}{\\partial W_O} \\end{aligned}$$ 在此我们重点讨论$\\frac{\\partial o_t}{\\partial W_O}$的计算。从上述符号表和公式我们可知$o_t$是一个$H \\times 1$的列向量，$W_O$是一个$H \\times K$的矩阵 2.计算$\\frac{\\partial L_t}{\\partial W_H}$ $$ \\frac{\\partial L_t}{\\partial W_H} = \\frac{\\partial L}{\\partial f(o_t,y)} \\cdot \\frac{\\partial f(o_t,y)}{\\partial o_t} \\cdot \\frac{\\partial o_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_H} $$ 由于$h_t$是个复合函数，$h_{t-1}$同样包含$W_H$，因此根据链式求导法则和导数的乘法法则可以推出以下公式 $$ \\begin{aligned} \\frac{\\partial h_t}{\\partial W_H} \u0026= \\frac{\\partial h_t}{\\partial W_H} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial W_H} \\\\ \u0026= \\frac{\\partial h_t}{\\partial W_H} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial W_H} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot \\frac{\\partial h_{t-2}}{\\partial W_H} + ... \\end{aligned} $$ ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:4","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"IRNN模型1 ","date":"2021-12-23","objectID":"/irnn/:0:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Novelty 专注于解决RNN模型中的梯度消失问题 使用单位矩阵来初始化RNN从而部分解决梯度消失问题 使用ReLU激活函数代替Sigmoid激活函数也用于解决梯度消失问题 ","date":"2021-12-23","objectID":"/irnn/:1:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"问题和动机 梯度消失和梯度爆炸的问题导致RNN模型难以学习到远距离依赖 过去的解决方法依赖于复杂的优化技术和网络架构 提出一种较为简单的方式进行优化 ","date":"2021-12-23","objectID":"/irnn/:2:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"过去的解决方法 用Hessian-Free来代替SGD2 3（Hessian-Free可以关注到曲率） 虽然效果上有改进，但并不常用，原因可能如下 比SGD基础上的方法更难实现 拟牛顿法需要更复杂的计算和内存开销 目标函数可能非凸 求得了更高精度的解可能不利于泛化 二阶法求得更高精度的解，但也需要更高精度的数据，深度学习数据本身存在很多随机误差，可能导致优化不稳定甚至失败 当使用了梯度裁剪4并关注了参数初始化5时，带动量的SGD与HF方法可以相当 最成功的改进LSTM 虽然能解决长距离依赖，但本文作者认为此结构并不是最优的结构 ","date":"2021-12-23","objectID":"/irnn/:3:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Idea 使用ReLU(Rectified Linear Units)激活函数 (trick)通过单位矩阵或者它的缩放版本来初始化RNN中的权重矩阵 作者认为以下工作与本文工作类似 本文作者认为其参数初始化方法与文献6中的方法相同，主要区别在于本文仅仅将单位矩阵用在初始化上，而且使用了ReLU激活函数。 scaled identity initialization同样在文献7中被提出，但没使用ReLU激活函数。 本文工作与文献8研究正交矩阵初始化同样类似。 ","date":"2021-12-23","objectID":"/irnn/:4:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"实验 实验中发现了下述炼丹技巧 为LSTM设置更高的forget gate bias能更好的解决长距离依赖关系 参数应用高斯随机初始化时使用文献9的值时效果更好。 实验中IRNN中非递归权重使用均值为0，标准差为0.001的高斯分布来初始化。分别展示了IRNN，标准LSTM，使用tanh激活的RNN和使用ReLU激活的RNN在4个实验上的结果。 ","date":"2021-12-23","objectID":"/irnn/:5:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"The Adding Problem实验 输入由两部分组成，第一部分是0-1范围内均匀分布的值，第二部分是掩码，最后的结果是经掩码后的两值得和 通过在固定序列中随机抽取两个值，将其值求和作为输出，对此问题进行回归。使用MSE来评价效果，当一直预测值为1时，MSE为0.1767。实验结果如下所示 可以发现IRNN呈现出与LSTM相当的效果，但带有ReLU激活的RNN效果为何一直这么差? ","date":"2021-12-23","objectID":"/irnn/:5:1","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"MNIST Classification from a Sequence of Pixels实验 实验通过将图片的784个像素顺序输入网络中，在完全学习完784个像素后，再进行图片的分类。一次每个网络的循环步长为784。除了对原始图片进行预测的实验之外，还对图片的像素进行固定的随机重新排列后进行了第二次实验。 可以发现，IRNN性能最好，超过LSTM，但带有ReLU激活的RNN效果为何一直这么差? ","date":"2021-12-23","objectID":"/irnn/:5:2","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Language Modeling实验 4层IRNN与LSTM表现相当(同时LSTM的参数是单层IRNN的四倍) ","date":"2021-12-23","objectID":"/irnn/:5:3","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Speech Recognition实验 在该实验中作者发现使用单位矩阵初始化效果较差，本文猜测的原因如下 正常IRNN很难忘记过去的信息 难以专注当前的输入 我觉得等于没说，根据模型和结果猜原因 因此本文提出了补救办法，并认为其可以作为不需要长距离依赖时模型的补救办法 用一个小标量和单位矩阵的乘积来初始化(本文中使用的是0.01I) iRNN模型与LSTM相当，比标准RNN效果好 ","date":"2021-12-23","objectID":"/irnn/:5:4","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"论文Insight 使用单位矩阵初始化循环参数(可能缓解梯度爆炸的问题) 使用ReLU激活(可以缓解梯度消失的问题) 二者共同作用可使得RNN模型性能得到改善 在不需要长期依赖时，可以使用小标量与I的乘积来进行初始化，类似于LSTM中的遗忘门机制 ","date":"2021-12-23","objectID":"/irnn/:6:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"思考中的问题 超参初始化问题，xariv，He，和本文中9提到的初始化的关系和效果 单位矩阵在模型中起到的作用(推导反向传播) 使用ReLU激活的RNN效果比tanh还差的原因 https://arxiv.org/abs/1504.00941 (A Simple Way to Initialize Recurrent Networks of Rectified Linear Units) ↩︎ Learning recurrent neural networks with Hessian-Free optimization. In ICML, 2011. ↩︎ Deep learning via Hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning, 2010. ↩︎ On the difficulty of training recurrent neural networks. ↩︎ On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning, 2013. ↩︎ Learning longer memory in recurrent neural networks ↩︎ Parsing with compositional vector grammars ↩︎ Exact solutions to the nonlinear dynamics of learning in deep linear neural networks ↩︎ Random Walk Initialization for Training Very Deep Feedforward Networks ↩︎ ↩︎ ","date":"2021-12-23","objectID":"/irnn/:7:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["运维"],"content":"linux配置 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:0:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"网络配置 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:1:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"缺少ifconfig等工具时 下载net-tools包 目前iproute2已逐渐取代net-tools工具包，成为系统自带的网络工具，iproute2命令包主要是以ip作为前缀的一些命令 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:1:1","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"简单路由配置 显示当前路由 route -n 添加一条路由route add -net *.*.*.* gw *.*.*.* netmask *.*.*.* dev eth0 删除一条路由route del -net *.*.*.* gw *.*.*.* netmask *.*.*.* dev eth0 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:1:2","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"多网卡路由配置 可查询此链接linux网络命令 查看linux网络命令的使用 使用策略路由 通过配置双路由表来配置不同网域使用不同路由表 具体方法 查看路由表存储情况 cat /etc/iproute2/rt_tables ​ 输出情况如下 # # reserved values # 255 local #本地路由表 254 main #主路由表，不加设定时我们增加的路由规则都设置于此 253 default #存放默认路由规则。注意增加默认规则时若没有指定路由表那还是存在于main表中 0 unspec # # local # #1 inr.ruhep route -n命令查看的是main路由表 查看对应路由表 ip route show table main ip route show table 254 修改rt_tables文件向指定路由表添加或删除规则使用ip route命令 ip route add 192.168.80.0/24 via 192.168.20.20 table 251 ip route add 192.168.80.0/24 via 192.168.30.20 table 252 ip route del 192.168.80.0/24 via 192.168.30.20 table 251 ip route del 192.168.80.0/24 via 192.168.30.20 table 252 快速删除某一特定路由或者路由表可使用ip route flush #清除192.168.0.0的路由信息 ip route flush 192.168.0.0 #清空main路由表 ip route flush table main 查看路由表策略 # ip rule show 或者 # ip rule ls 0: from all lookup local 32766: from all lookup main 32767: from all lookup default 创建策略( pref 越小越先匹配) #根据源地址决定路由表 ip rule add from 192.168.10.0/24 table 100 pref 10 ip rule add from 192.168.20.20 table 110 pref 100 #根据目的地址决定路由表 ip rule add to 192.168.30.0/24 table 120 ip rule add to 192.168.40.0/24 table 130 #根据网卡设备决定路由表 ip rule add dev eth0 table 140 ip rule add dev eth1 table 150 #此外还可以根据其他条件进行设置，例如tos等等 删除策略 #根据明细条目删除 ip rule del from 192.168.10.10 #根据优先级删除 ip rule del prio 32765 #根据表名称来删除 ip rule del table wangtong 或者使用netplan1配置，具体 修改/etc/netplan/*.yaml文件，示例如下 network: version: 2 renderer: networkd ethernets: ens3: addresses: - 192.168.3.30/24 dhcp4: no routes: - to: 192.168.3.0/24 via: 192.168.3.1 table: 101 routing-policy: - from: 192.168.3.0/24 table: 101 priority: 10 nameservers: address: [192.168.3.1] gateway4: 192.168.3.1 ens5: addresses: - 192.168.5.24/24 dhcp4: no routes: - to: 0.0.0.0/0 via: 192.168.5.1 table: 102 - to: 192.168.5.0/24 via: 192.168.5.1 table: 102 routing-policy: - from: 192.168.5.0/24 table: 102 priority: 10 当设置了gateway4之后将在默认的default表自动设置对应的默认路由 当render设置为NetWorkManager时,可能会提示不能设置没有默认路由的路由表,将render一行删除即可 缺省render的情况下，默认使用systemd-networkd 在改好相关配置后，需要systemctl restart相关服务 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:1:3","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"cli工具 NetWorkManager使用的cli工具是nmcli，服务是NetWorkManager.service systemd-networkd使用的cli工具是networkctl，服务是systemd-networkd.service ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:1:4","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"设置开机启动自动运行的脚本 旧的版本中可以直接编辑/etc/rc.local添加开机启动脚本，而新版本这个功能默认是禁用的 Ubuntu20.04按下操作开启rc-local.service 给/etc/rc.local文件执行权限chmod +x或者chmod 755 vi /lib/systemd/system/rc-local.service添加如下代码 [Install] WantedBy=multi-user.target 启动服务systemctl enable rc-local Ubuntu18.04使用rc.local来对文件和服务命名,因此基本只需将20.04命令中的rc-local改为rc.local即可 给rc.local文件执行权限chmod +x或者chmod 755 vi /lib/systemd/system/rc.local.service,添加如下代码 [Install] WantedBy=multi-user.target Alias=rc-local.service 启用服务systemctl enable rc.local.service ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:2:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"驱动 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:3:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"内核更新后重新安装ubuntu驱动 sudo dpkg --list | grep nvidia-*或者cat /proc/driver/nvidia/version查看gpu驱动版本 sudo apt-get autoremove --purge nvidia-*删除nvidia相关包 sudo apt-get install linux-headers-$(uname -r)安装新内核的linux-headers，用于编译各种内核模块 sudo apt-get install nvidia-drivers-4**安装新的nvidia驱动 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:3:1","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"ubuntu更新内核 uname或hostnamectl查看内核版本 可使用uname --help查看具体用法 dpkg --get-selections | grep linux或者 dpkg --list |grep linux查看已安装的内核版本 sudo apt-get install linux-image-version-generic安装Linux镜像 sudo apt-get install linux-image-extra-version-generic安装新内核的额外驱动 sudo apt-get install linux-headers-version-generic安装linux-headers sudo apt-mark hold linux-image-version-generic固定内核镜像 sudo apt-mark hold linux-image-extra-version-generic固定内核的额外驱动 sudo apt-mark hold linux-headers-version-generic固定linux-headers ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:4:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"grub相关 开机时按shift可进入grub界面 sudo vim /etc/default/grub修改grub文件可修改默认启动项 修改完之后要用sudo update-grub来更新grub /boot/grub/grub.cfg包含了各个启动项的详细信息 若重装系统时，开机出现引导失败，进入grub rescue模式，可尝试写入启动盘时使用dd模式 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:5:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"ubuntu图形界面失效 ctrl+alt+F2~F6可进入tty命令行界面 如果出现图形界面黑屏，但可以进入tty模式，考虑如下方法解决 修改/boot/grub/grub.cfg，在quiet splash后面添加nomodeset 修改/etc/default/grub,在quiet splash后面添加nomodeset ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:5:1","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"用户管理 涉及四个文件，主要需要修改passwd文件 文件 含义 /etc/shadow 加密的用户账户信息 /etc/passwd 用户账户信息 /etc/gshadow 隐藏的组账户信息 /etc/group 定义了用户属于哪个组 useradd命令，可通过man useradd查看用法 参数 含义 -d 目录 指定一个用户目录，若目录不存在，需要-m参数创建主目录 -g 用户组 指定用户所属的用户组 -G 用户组，用户组 指定用户所属的附加组 -s shell 指定用户登录的shell -u 用户号 指定用户的用户号 -m 自动建立用户的登录目录 -M 不自动建立用户的登录目录 -n 不自动建立以用户名为名的用户组 新建一个普通用户 useradd -d /home/username -m username -s /bin/bash passwd username 新建一个管理员用户 useradd -d /home/username -m username -s /bin/bash -g sudo passwd username userdel命令，删除用户，加-r参数也删除用户主目录 usermod命令，参数与useradd一致，含义变为修改 修改/etc/sudoers文件，为用户添加sudo权限，添加如下 username ALL = (ALL) ALL ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:6:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"安全相关 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:7:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"SSH安全 具体参阅archwiki ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:7:1","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"ufw防火墙设置 sudo ufw status查看防火墙状态 sudo ufw default deny默认拒绝 sudo ufw allow from 192.168.0.0/24允许某个ip段访问 sudo ufw all 22/tcp/udp允许22端口的tcp或者udp访问，若不加tcp或者udp则都允许访问 sudo ufw all ssh允许ssh的端口访问 sudo ufw limit ssh限制ssh的访问，禁用过去30秒尝试启动6个或以上的IP连接 \\etc\\ufw\\user.rules存储了规则，\\etc\\ufw\\before.rules可以设置黑名单 详细可参阅官方文档 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:7:2","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"nvim配置 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:8:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"linux 从github拉取配置文件并放在.config目录下，命名为nvim git clone https://github.com/leviathanion/nvim-config.git .config/nvim 下载packer插件管理工具2，然后进入nvim运行:PackerSync 可通过:checkhealth查看各插件情况 在.bashrc或者.zshrc中添加以下代码，使得root权限也可用 alias sudonvim = \"sudo -E nvim\" ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:8:1","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"windows 与linux类似，将配置文件放到c:user/appdata/local/nvim中 ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:8:2","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["运维"],"content":"zsh配置conda cd anaconda的安装目录下的bin目录 执行conda init zsh conda init zsh https://netplan.io/examples/  ↩︎ https://github.com/wbthomason/packer.nvim  ↩︎ ","date":"2021-12-21","objectID":"/linux%E9%85%8D%E7%BD%AE/:9:0","tags":["运维"],"title":"linux配置","uri":"/linux%E9%85%8D%E7%BD%AE/"},{"categories":["命令"],"content":"关于git命令的介绍","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"git使用1 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:0:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"基本命令 创建仓库git init 把文件添加到仓库git add 全部添加 git add . 把文件提交到仓库git commit 可以多次git add后git commit 命令之后必须加参数 -m \"注释\"来提交 查看目前仓库情况git status git diff将本地文件与暂存区的文件进行比较 git log 查看文件历次commit提交的情况 commit后面的数字是提交的编号 HEAD表示当前版本,HEAD^表示上个版本，HEAD^^表示上上个版本 HEAD~100表示往上的100个版本 git reset 用法：git reset --hard HEAD^表示回退到前一个版本 发现最早的版本记录没了 此时可以通过git reset --hard 之前的版本号来恢复操作 git reflog 可以查看所有分支的所有操作记录（包括已经被删除的 commit 记录和 reset 的操作） 例如执行 git reset --hard HEAD~1，退回到上一个版本，用git log则是看不出来被删除的commitid，用git reflog则可以看到被删除的commitid，恢复到被删除的那个版本。 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:1:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"小结 创建git仓库使用git init命令 提交文件，对多个文件git add之后git commit -m \"\" 查看仓库情况，使用git status可以查看文件被修改，add了修改文件的情况，仓库没有发生变化三种状态 如果git status显示文件被修改，但还没被add，可以用git diff查看修改情况 查看版本提交历史情况使用git log HEAD表示当前版本,HEAD^表示上个版本，HEAD^^表示上上个版本HEAD~100表示往上的100个版本 版本回退使用git reset --hard commit_id/HEAD^^/HEAD~100 重返未来，使用git reflog查看提交历史命令 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:1:1","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"暂存区和master分支 git暂存区和master分支的概念 git是管理修改的，git add之后将文件添加到暂存区,如果没有将修改提交到暂存区，对文件的更改也不会上传到master分支上 如果对文件修改错误，但还没addgit checkout --readme.txt让文件恢复到最后一次git add或git commit的状态 如果错误的文件已经git add，使用git reset HEAD readme.txt可将暂存区的文件恢复至版本库文件的版本 当改乱了某文件内容，想直接丢弃工作区修改时，使用git checkout -- file 如果已经add了，文件在暂存区中，使用git reset HEAD \u003cfile\u003e 回到第一步，然后再按第一步操作 如果已经commit了，使用git log 然后使用git reset命令 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:2:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"删除文件 如果想要删除版本库里的文件，先删除本地文件,然后git rm删除文件，然后git commit git rm \u003cfile\u003e将文件从暂存区和工作区中删除 git rm --cached \u003cfile\u003e从暂存区移除，但仍然保留在当前工作目录中，仅是从跟踪清单中删除 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:3:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"链接远程库 git remote add origin ***关联远程库，***代表远程库的地址，可以为git或者https,git地址速度块 首次s git remote -v查看远程仓库 git push --force origin master或者git push -f origin master强制覆盖远程分支 git checkout -b a origin/a从远程a分支拉取分支信息到本地a分支 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:4:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"分支创建和基本使用 创建分支git branch dev 切换到分支git checkout dev 查看分支情况git branch 将别的分支合并到当前分支git merge 分支名 如果存在冲突，需要解决冲突 修改完冲突文件后使用git add .命令 再使用git commit命令，可以不带-m 参数添加注释 删除分支git branch -d 分支名 在操作中添加-r参数，代表对远程仓库进行分支操作 对一个分支的本地文件操作不会影响到另一分支的本地文件情况 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:5:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"Git进阶 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:6:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"分支管理 git rebase branch变基操作 将branch分支中的commit放到当前的commit之前，合并为同一分支 git commit --amend可以编辑当前的commit信息 git rebase -i HEAD~3 编辑前三个commit 通过squash可用于多个commit信息的合并 通过edit可用于历史commit信息的编辑 编辑完文件后仍需要多次运行git commit --amend来编辑多次历史提交信息 每次运行上述命令之后，需要git rebase --continue代表保存并进行下一条 --abort参数可放弃当前操作，例如git rebase --abort git push --set-upstream origin newbranch将本地分支newbranch与远程分支newbranch关联 origin/main和是远程分支在本地分支更新的克隆 git pull操作是git fetch和git merge origin/***两个命令的集合 git fetch命令使用远程分支更新本地origin/***分支 git merge origin/***将origin/***分支合并到当前***分支 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:6:1","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"git status 中文文件名编码问题 在默认设置下，中文文件名在工作区状态输出，查看历史更改概要，以及在补丁文件中，文件名的中文不能正确地显示，而是显示为八进制的字符编码，示例如下： \"assets/\\346\\234\\272\\345\\231\\250\\345\\255\\246\\344\\271\\240\\347\\232\\204\\345\\271\\266\\350\\241\\214\\347\\255\\226\\347\\225\\245/\" \"content/posts/\\346\\234\\272\\345\\231\\250\\345\\255\\246\\344\\271\\240/\" 通过将Git配置变量 core.quotepath 设置为false，就可以解决中文文件名称在这些Git命令输出中的显示问题 git config --global core.quotepath false 将此改到github action文件中，同样可以解决hugo模板lastmod显示不正常的问题 https://www.liaoxuefeng.com/wiki/896043488029600  ↩︎ ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:6:2","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"关于linux的部分命令介绍","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"linux命令 小技巧： ctrl + shift + =放大终端字体 ctrl + -缩小终端字体 tab 若不存在歧义，则自动补全 若tab目标太多不能自动补全时，按两次tab可显示符合条件的目录 ctrl + c可退出 ctrl + a快速回到行首，ctrl + e快速回到行尾 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:0:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"1.linux基本命令 序号 命令 对应英文 作用 01 ls list 查看当前文件夹内容 02 pwd print work directory 查看当前所在文件夹 03 cd [目录名] change directory 切换文件夹 04 touch [文件名] touch 如果文件不存在，则新建文件 05 mkdir [目录名] make directory 创建目录 06 rm [文件名] remove 删除指定文件名 07 clear clear 清屏 08 cp 源文件 目标文件 copy 复制文件或目录 09 mv 源文件 目标文件 move 移动文件或者目录/文件或者目录重命名 10 cat [文件名] 查看文件所有内容 11 more [文件名] more 按页查看文件内容 12 grep [文本] [文件] 在文件中查找匹配的文本 注意事项 Linux下区分大小写 不加参数的rm只能删除文件，不能删除文件夹 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:1:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"2.Linux终端命令格式 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:2:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"2.1 终端命令格式 command [-options] [parameter] 说明： command:命令名 [-options]:选项 多选项时可单独列出 -a -b -c等，也可合在一起-abc [parameter]:传递的参数 []代表可选 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:2:1","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"2.2 查询命令帮助信息 2.2.1 –help command --help 说明： 显示command命令的帮助信息 2.2.2 man mam command 说明： 查询command命令的使用手册 man是manual的缩写，是linux提供的一个手册，包含了绝大多数命令的详细使用说明 使用man的快捷键 操作键 功能 空格 显示手册的下一屏 回车键 一次滚动手册页的一行 b 回滚一屏 f 前滚一屏 q 退出 /word 搜索word字符串 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:2:2","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.linux文件和目录命令 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.1命令总结 查看目录内容 ls tree以树状图列出文件目录结构 切换目录 cd 创建和删除操作 touch mkdir rm 拷贝和移动文件 cp move 查看文件内容 cat more grep ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:1","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.2 ls命令详细用法 1. Linux文件特点 以.开头的文件为隐藏文件，需要加参数-a才能显示 .代表当前目录 ..代表上级目录 2. ls命令常用选项 选项 含义 -a 显示指定目录下的所有文件和目录，包括隐藏 -l 以列表方式显示文件的详细信息 -h 配合-l 以人性化方式显示文件大小 3.ls通配符使用 通配符 含义 举例使用 * 代表任意个字符 ls 1*,ls 12*.txt,ls *.txt ？ 代表任意一个字符 ls 12?.txt,ls ?12.txt [] 创建字符组，表示可以匹配字符组中的任意一个 ls [123][123].txt [a-f] 匹配a到f所有字符中的一个 ls [1-3].txt,ls [a-f].txt ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:2","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.3 cd命令详细用法 1. cd命令常用用法 命令 cd 进入到用户家目录 cd ~ 进入到用户家目录 cd .. 返回到上级目录 cd - 在最近两次的工作目录中来回切换 2. 绝对路径和相对路径 绝对路径：输入路径开头是/或者~，表示从根目录或家目录开始的具体目录位置 相对路径：输入路径开头不是/或者~，表示相对当前目录的目录所在位置 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:3","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.4 创建和删除命令详细用法 1.touch命令 创建文件或修改文件时间 若文件不存在，则创建一个空白文件 若文件已经存在，则修改文件的末次修改日期 2.mkdir命令 创建一个新的目录 mkdir -p循环创建多个目录 例如**mkdir -p a/b/c/d** mkdir创建的目录路径下不能有与之同名的目录或文件 3.rm命令 rm删除直接从磁盘上删除文件，并不会放到垃圾桶，无法恢复 选项 含义 -f 强制删除，忽略不存在的文件，且不提示 -r 递归删除一个目录下所有内容，删除文件夹时使用 rm命令也可以使用通配符 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:4","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.5 拷贝和移动命令 1. tree命令 tree命令可以以树状图列出文件目录结构 不加参数显示当前目录的树形结构 tree [目录]显示指定目录的结构 参数 含义 -d 只显示目录 2.cp命令 cp命令的功能是将给出的文件或目录复制到另一个文件或者目录中 cp 源路径/源文件名 目标路径/目标名 同时可以给目标重命名 cp并不能直接复制目录 cp ~/documents/123.txt ./readme.txt 选项 含义 -i 提示覆盖信息 -r 复制目录 3. mv命令 mv命令可以移动文件和重命名 例：mv ./desktop/123.txt ./documents/(234.txt) ps:123.txt会消失,()代表可选 mv 123(.txt) 2(.txt)可以重命名 注意事项： 假设已经存在了234.txt，则会用123.txt覆盖234.txt 参数 含义 -i 覆盖文件前提示 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:5","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.6 查看和查找文件内容 1. cat 命令 cat 123.txt可以显示123.txt的所有内容 选项 含义 -b 按非空行编号并输出 -n 按所有行编号并输出 2.more命令 more 1.txt可以实现一屏的内容 操作键 功能 空格 显示手册的下一屏 回车键 一次滚动手册页的一行 b 回滚一屏 f 前滚一屏 q 退出 3.grep命令 grep 文本内容 文件 grep \"hello world\" 1.txt当匹配的字符串含空格，则用“”框起来 选项 含义 -n 显示匹配行及行号 -v 显示不包括匹配文本的所有行 -i 忽略大小写 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:6","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.7 重定向和管道 echo命令会在终端中显示参数指定的文字，通常和重定向联合使用 1.重定向 \u003e和» 可以将命令的执行结果重定向到一个文件中 将本应输出在终端的结果输出或追加到其他文件上 \u003e输出到文件中，覆盖原文件内容 \u003e\u003e输出到文件中，追加到原文件最后 2. 管道| ls -lh | more类似这样的用法 将第一个命令的输出作为第二个命令的输入 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:7","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.远程管理命令 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.1命令总结 序号 命令 英文 作用 01 shutdown 选项 时间 shutdown 关机/重新启动 02 ifconfig configure a network interface 查看/配置计算机当前的网卡配置 03 ping IP地址 ping 检测到目标ip地址的连接是否正常 04 ssh [-p port] user@remote ssh客户端命令 连接远程ssh服务器 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:1","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.2 关机和重启命令 shutdown可以安全关闭或者重新启动系统 选项 含义 -r 重新启动 提示： 不指定选项和参数时，默认1分钟后关闭电脑 shutdown shutdown now shutdown -r now shutdown 20:25 //20：25时关机 shutdown +10 //十分钟后关机 shutdown -c//取消关机动作 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:2","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.3 查看和配置网卡信息 4.3.1 网卡和IP地址 网卡是一个专门负责网络通讯的硬件设备 ip地址是设置在网卡上的地址信息，不能重复 电脑是电话，网卡是SIM卡，IP地址是电话号 4.3.2 ifconfig命令 ifconfig可以查看/配置计算机当前的网卡配置信息 # 查看网卡配置信息 $ ifconfig # 关闭网卡 $ ifconfig 网卡 down # 打开网卡 $ ifconfig 网卡 up # 查看网卡对应的ip地址 $ ifconfig| grep inet 127.0.0.1被称为本地回环/环回地址，一般用来测试本机网卡是否正常 4.3.3 ping命令 # 检测到目标主机是否连接正常 $ ping ip地址 # 检查本地网卡是否工作正常 $ ping 127.0.0.1 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:3","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.4 远程登录和复制文件 4.4.1 ssh基础 通过ssh客户端 登录管理 ssh服务器 在linux/macOS上 ssh服务器和ssh客户端都是自动安装的，只有windows需要自己安装 ssh客户端是一种使用Secure Shell(SSH)协议连接到远程计算机的软件程序 SSH数据传输是压缩且加密的 1）域名和端口号 1.域名 www.baidu.com类似如此的名字，是IP地址的别名，方便用户记忆 2.端口号 IP地址：通过IP地址找到计算机 端口号：通过此找到计算机上的应用程序 常见服务器端口列表 序号 服务 端口号 01 SSH服务器 22 02 Web服务器 80 03 HTTPS 443 04 FTP服务器 21 如果是默认端口号，在连接时可以省略 否则 IP地址：端口号 2）ssh客户端的简单使用 ssh [-p port] user@remote user是远程机器上的用户名，不指定的话默认为当前用户 remote是远程机器的地址，可以是ip/域名或者是别名 port是SSH Server监听的端口，如果不指定，就为默认值22 提示： 使用exit退出当前用户的登录 4.4.2 scp命令 scp命令，是一个在linux下进行远程拷贝文件的命令 scp地址格式与ssh基本相同，但在指定端口时，使用-P而不是-p # 把本地目录下的01.py复制到 远程 家目录下的Desktop/01.py # `:`后面的路径不是绝对路径，则以用户的家目录作为参照路径 scp -p port 01.py user@remote:Desktop/01.py # 把远程 家目录下的Desktop/01.py 辅助到 本地当前目录下的01.py scp -p port user@remote:Desktop/01.py 01.py #加上r可以传送文件夹 选项 含义 -P 加端口 -r 拷贝目录 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:4","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"}]