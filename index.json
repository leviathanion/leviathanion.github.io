[{"categories":["机器学习"],"content":"矩阵求导 ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:0:0","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"分子布局和分母布局 标量，向量，矩阵之间的求导，相对于标量对标量的求导，需要考虑一个额外的因素，就是求导之后的布局，例如标量对列向量求导之后是按照列向量排列还是按照行向量排列并没有一个确切的规定。因此这里我们引入分子布局和分母布局的概念。 分子布局指的是求导之后的排列方式和维度以分子为主 例如对于列向量$\\mathbf{y}$而言，$\\frac{\\partial \\mathbf{y}}{x}$，按照分子布局，得到的结果也是列向量。 一个标量$y$对 $m\\times n$维的矩阵 $X$求导，其结果为$n\\times m$维。 一个$m$维的列向量$\\mathbf{y}$对一个$n$维的行向量$\\mathbf{x}$求导，其结果为 $$ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}= \\begin{Bmatrix} \\frac{\\partial y_1}{\\partial x_1} \u0026 \\frac{\\partial y_1}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_1}{\\partial x_n}\\\\ \\frac{\\partial y_2}{\\partial x_1} \u0026 \\frac{\\partial y_2}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_2}{\\partial x_n}\\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial y_m}{\\partial x_1} \u0026 \\frac{\\partial y_m}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_n}\\\\ \\end{Bmatrix} $$ 是一个$m \\times n$维的矩阵，也被叫做雅可比矩阵 分母布局指的是求导之后的排列方式和维度以分母为主 对于列向量$\\mathbf{y}$而言，$\\frac{\\partial \\mathbf{y}}{x}$，按照分母布局，得到的结果是行向量 一个标量$y$对 $m\\times n$维的矩阵$X$求导，其结果为$m\\times n$维。 一个$m$维的列向量$\\mathbf{y}$对一个$n$维的行向量$\\mathbf{x}$求导，其结果为 $$ \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}= \\begin{Bmatrix} \\frac{\\partial y_1}{\\partial x_1} \u0026 \\frac{\\partial y_2}{\\partial x_1} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_1}\\\\ \\frac{\\partial y_1}{\\partial x_2} \u0026 \\frac{\\partial y_2}{\\partial x_2} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_2}\\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ \\frac{\\partial y_1}{\\partial x_n} \u0026 \\frac{\\partial y_2}{\\partial x_n} \u0026 \\cdots \u0026 \\frac{\\partial y_m}{\\partial x_n}\\\\ \\end{Bmatrix} $$ 是一个$n \\times m$维的矩阵，也被叫做梯度矩阵 因此对于标量，向量和矩阵的求导可按下述表格来定义： 自变量/因变量 标量$y$ $m$维列向量$\\mathbf{y}$ $m \\times n$维矩阵$Y$ 标量$x$ / $\\frac{\\partial \\mathbf{y}}{\\partial x}$ 分子布局：$m$维列向量 分母布局：$m$维行向量 分子布局：$m \\times n$维矩阵 分母布局：$n \\times m$维矩阵 $m$维列向量$\\mathbf{x}$ $\\frac{\\partial y}{\\partial \\mathbf{x}}$ 分子布局：$m$维行向量 分母布局：$m$维列向量 $\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}$ 分子布局：$m \\times n$维矩阵 分母布局：$n \\times m$维矩阵 / $m \\times n$维矩阵$X$ $\\frac{\\partial y}{X}$ 分子布局：$n \\times m$维矩阵 分母布局：$m \\times n$维矩阵 / / ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:1:0","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"求导方法–微分法 ","date":"2022-01-19","objectID":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/:2:0","tags":["机器学习","矩阵求导"],"title":"矩阵求导","uri":"/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"},{"categories":["机器学习"],"content":"机器学习中的分布式的并行优化 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:0:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"简介 机器学习的并行计算指的是对机器学习过程中遇到的计算问题采用并行计算的方式进行合适的加速，缩短训练所需的时间。此处所指的时间既指物理时间，也指CPU时间或者GPU时间。 和其他大数据计算问题类似，有两种途径来实现对计算的并行加速，一种是向单一机器添加更多计算资源，可以称之为纵向扩展，另外一种是类似分布式系统一样，在系统中添加更多的节点，节点可能是CPU，GPU甚至是单机环境等等，可以称之为横向扩展。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:1:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"纵向扩展 CPU或者GPU时间的缩短主要应用纵向扩展，通过对特定问题设计特定的CPU或者GPU模块及指令集等，通过硬件加速机器学习算法中的特定问题。虽然机器学习近年来涌现出各种各样互不相同的算法，但是对于底层的计算而言，这些算法所使用的数据操作的本质基本相似，都是线性代数中的基本操作，是对向量，矩阵，张量的基本计算，因此涌现出了很多种加速方法和实现方案。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:2:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"主要策略 纵向扩展中的常用方法是，添加可编程GPU。最初，GPU用于机器学习的应用受到限制，因为GPU使用纯SIMD（单指令多数据）模型，不允许内核执行不同的代码分支；所有线程都必须执行完全相同的程序。 后来出现的通用GPU，即可以执行任意代码的GPU。这些产品可以作为加速器添加到传统机器上，加快了机器学习的训练效率。例如Nvidia的Titan V和Tesla V100显卡就可以显著加速机器学习的学习和训练。 除了使用通用GPU加速之外，还可以使用专用集成电路（ASIC）来加速机器学习，专用集成电路主要通过高度优化的设计实现特定功能。 在最近几代产品中，即使是通用CPU也增加了向量指令的可用性和宽度，以加速计算密集型问题（如机器学习算法）的处理。这些指令是矢量指令，是AVX-512系列的一部分，具有增强的字变量精度并支持单精度浮点运算。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:2:1","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"横向扩展 采用纵向扩展的优化方法之外，还可以对单一问题进行拆解，使其可以在多处理器，多显卡，多机的环境下进行计算，主要解决单处理器，单显卡，单机性能提升较慢，难以跟上深度学习中计算资源的需求问题，通过数量来加速计算。在工程实践中采用横向扩展的案例也并不少见。 采用横向扩展，单机设备的成本较低，同时易于增加算力。 单机设备在计算过程中发生故障时往往难以恢复，对于多机环境而言，单个处理器发生故障时，系统仍然可以通过启动部分恢复（例如，基于通信驱动的检查点1或部分重新计算2）继续运行。 单处理器环境下，对于大规模数据的读取可能有IO瓶颈。 横向扩展的一个主要挑战是，并非所有ML算法都适用于分布式计算模型。 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:3:0","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"数据并行和模型并行 数据并行将原始数据分配到不同的工作节点上并行训练。其中，每一个工作节点使用不同的部分数据，但是都拥有完整的模型，工作节点之间一般会同步自己的局部梯度信息，再进行汇总，得到整体的更新结果。数据并行依赖于优化算法的选择。 模型并行一般是由于模型太大，单机无法储存，将模型的不同部分放在不同的节点上进行训练，常用的方式是每一个节点均使用相同的数据，但是只使用模型的一部分来进行。因此模型并行依赖于模型的设计。 在实际应用过程中，数据并行更为常见。 模型并行和数据并行模型并行和数据并行 \" 模型并行和数据并行 ","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:3:1","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["机器学习"],"content":"主要策略 横向扩展组成的分布式机器学习系统中，性能主要取决于三部分，计算瓶颈，IO瓶颈和通信瓶颈。 由于IO和通信的限制，虽然机器数量在不断增加，但加速效果并不是线性提升，在增加数量过程中，会产生性能损耗，比如，增加了十倍的机器，理想训练速度能够增加十倍，实际上往往却只增加了一两倍，性价比很低。这实际上是因为IO和通信瓶颈所导致的。 期望能够提高加速比，使得分布式机器学习可以更快，就需要降低通信和IO时间开销的同时，加快计算性能，才能提升计算的时间占比，使得性能损耗更小。 下面从这三个点来展开横向扩展的优化策略。 通信优化 通信上，一方面提升通信速度，比如通信拓扑的改进，通信步调和频率的优化，另一方面也可以减少通信内容和次数，比如梯度压缩和梯度融合技术等。 通信方式 Share memory：同的处理器共享一块内存，没办法同时用很多处理器进行工作 Message passing：有多个节点，节点的处理器之间是可以采用共享内存，节点之间不能共享内存。节点之间可以网线相连接也可以使用 TCP/IP 进行消息传递。需要注意的是，采用Message passing方法时通常使用MPI的标准库来进行并行通信。根据节点的协调方式可以分为两类 点对点(point-to-point)通信，这是高性能计算(HPC)中最常使用的模式，通常是与其最近的邻居进行通信，每个实例都是单发送方，单接收方 集合(collective)通信，也可以叫做C/S架构，存在多个发送方和接收方。 通信步调 Bulk Synchronous Parallel（BSP） 是最简单的模型，其中程序通过同步每个计算和通信阶段来确保一致性。遵循BSP模型的程序示例是MapReduce。优点是ML程序可以保证输出正确的解决方案。缺点是，完成的节点必须在每个同步障碍处等待，直到所有节点完成，这会导致在某些节点进度比其他工作人员慢的情况下产生开销。 Stale Synchronous Parallel（SSP）通过允许速度更快的节点向前移动一定数量的迭代来缓解同步开销。如果超过此数字，则暂停所有工作进程。节点在缓存的数据版本上操作，并且仅在任务周期结束时提交更改，这可能会导致其他节点在过时的数据版本上操作。数据SSP的主要优点是它仍然享有强大的模型收敛保证。然而，缺点是，当陈旧性变得太高时（例如，当大量机器减速时），收敛速度会迅速恶化。 Approximate Synchronous Parallel（ASP）限制了参数的不准确程度。这与SSP形成对比，SSP限制了参数的过时程度。一个优点是，每当聚合的更新无关紧要时，服务器都可以延迟同步。一个缺点是，很难选择定义哪些更新重要，哪些更新不重要的参数。 Barrierless Asynchronous Parallel /Total Asynchronous Parallel (BAP/TAP) 让节点并行通信，而无需彼此等待。其优点是通常可以获得尽可能高的加速比。一个缺点是，模型可能收敛缓慢，甚至发展不正确，因为与BSP和SSP不同，误差随延迟而增长。 MapReduce和Spark MapReduce是C/S架构，Server可以把信息广播到worker节点。Server先定义一个 Map 操作，这个 Map 操作是由worker节点完成，然后worker把结果传回client并处理，这个叫做reduce。梯度下降可以用 MapReduce 进行并行化。并行化的过程中，数据被分给 worker 进行计算。每一个梯度下降过程包含一个广播、map和一个 reduce 操作。 MapReduce的主要问题有两个，一是原语的语义过于低级，直接使用其来写复杂算法，开发量比较大；另一个问题是依赖于磁盘进行数据传递，性能跟不上业务需求。 为了解决MapReduce的两个问题，Matei在3中提出了一种新的数据结构RDD，并构建了Spark框架。Spark框架在MR语义之上封装了DAG调度器，极大降低了算法使用的门槛。 Spark是基于内存进行数据处理的，MapReduce是基于磁盘进行数据处理的。 DAG计算模型在迭代计算上还是比MapReduce的效率更高。 MapReduce中，reduce任务需要等待所有map任务完成后才可以开始；在Spark中，分区相同的转换构成流水线放到同一个任务中运行。 较长时间内spark几乎可以说是大规模机器学习的代表，直至后来李沐完善了参数服务器，开拓了大规模机器学习的领域以后，spark才暴露出一点点不足。 参数服务器 参数服务器的概念最早大概可以追溯到Alex Smola于2010年提出的并行LDA的框架，其采用一个分布式的Memcached作为存放参数的存储，用于在分布式系统不同的Worker节点之间同步模型参数，而每个Worker只需要保存它计算时所依赖的一小部分参数。 在此之后，PS又有了很多改进，其中又以李沐2014年提出的ps-lite4(所谓第三代PS架构)为主要代表，也进一步加快了业界广泛使用参数服务器的步伐，在广告，推荐等各领域内大放异彩，时至今日，依然在各大公司内发挥着重要作用。 ps-lite的主要架构示意图如下图所示。 ps-lite架构ps-lite \" ps-lite架构 其中，resource manager用来对当前的各个计算资源进行管理，可以直接利用资源管理组件如yarn、mesos或者k8s来实现，而底下的training data就是用来采集训练数据，在大规模场景下，一般需要类似GFS的分布式文件系统的支持，剩下的server group和worker group部分就是参数服务器的核心组件了。 Paraeter Server框架中，每个server都只负责分到的部分参数（server共同维持一个全局共享参数）。server节点可以和其他server节点通信，每个server负责自己分到的参数，server group 共同维持所有参数的更新。server manage node负责维护一些元数据的一致性，例如各个节点的状态，参数的分配情况。worker节点之间没有通信，只和对应的server有通信。 每个worker group有一个task scheduler，负责向worker分配任务。一个具体任务运行的时候，task schedule负责通知每个worker加载自己对应的数据，然后去server node上拉取一个要更新的参数分片，用本地数据样本计算参数分片对应的变化量，然后同步给server node；server node在收到本机负责的参数分片对应的所有worker的更新后，对参数分片做一次update。 从通信视角上看，其是一种比较朴素直观的算法过程，可以看成是reduce+broadcast的过程，先是将worker上的信息reduce到server节点上，之后server节点汇总了信息后，再broadcast到worker节点上去，完成了一次信息的处理过程，如下图所示。在这个结构中也能看到，worker之间不通信，而全部依赖于server节点，worker之间的通信能力未得到充分利用， 并且是单工通信，没有同时利用上行带宽和下行带宽，当参数非常稠密，需要通信的信息比较多时，server节点有可能成为瓶颈。 但是如果参数是高维稀疏，单机无法保存全部参数，且每个worker无需访问全部的参数的情况，如推荐中的百亿级feature的LR，LDA，小数据量的通信延迟较低，加上PS架构支持异步更新，可以减少阻塞，加快训练速度。粗略地说，原始的PS架构更适合稀疏超大模型，且更容易容灾，也因此在推荐领域内广泛应用。 ps-lite运行过程ps-lite运行过程 \" ps-lite运行过程 Ring All-Reduce PS架构虽然在很多领域内大放异彩，应用广泛，但是当模型稠密，需要大量交换信息的情况下，Server节点很容易成为瓶颈，限制了其作用，也因此有了将Ring AllReduce这一类通信方法应用到机器学习领域的尝试。 实际上，Ring AllReduce算法在高性能计算领域中已经有了比较长的历史，OpenMPI中至少在2007年就有了关于其的开源实现。然而机器学习领域内的对此知之甚少，更加不知道怎么利用其来加速分布式机器学习的速度。直到2016年，百度的研究人员首次尝试将Ring AllReduce算法应用到深度学习领域内，并在很多问题上取得了明显比PS架构更显著的加速效果，在深度学习领域取得了广泛的关注。 正如名字中所表达，**Ring AllReduce算法首先需要将集群内各个节点按照环状的形式排列，在这个环中，每一个节点都只接收其左邻居节点的信息，且都只发送信息给自己的右邻居节点。**在具体的通信内容和方式的组织上，大概可以分为两部分，第一部分，对于N个节点的集群，将每个节点上数据切分为N份，然后经过N-1轮的Reduce-Scatter过程。具体地，每一轮中，每个GPU将自己的一个chunk发给右邻居，并接收左邻居发来的chunk，并累加，经过这样的步骤，每一个节点都拥有一部分数据的最终结果。第二部分，与上部分相类似，进行N-1轮的AllGather过程，将每一个节点上的一部分的完整信息传递到所有节点上，经过此步骤，每一个节点上就拥有了所有数据的完整信息。 减少通信内容–梯度压缩 梯度压缩里有两大类主要的方案，一是梯度量化的方法，二是梯度稀疏化的方法。 梯度量化 模型量化等技术在模型推理上发展的相对成熟，也已经有很多成功的应用，可以有效的减少模型尺寸，降低模型推理成本","date":"2022-01-12","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/:3:2","tags":["机器学习","并行程序设计"],"title":"机器学习的并行策略","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5/"},{"categories":["RNNs","深度学习","机器学习"],"content":"RNN和LSTM ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:0:0","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"RNN ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:0","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"问题和动机 自然界中很多事物都是序列相关的，想要理解某一时刻，必须要获取过去时刻的信息 过去的模型难以捕获序列信息 ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:1","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"idea 通过一个状态变量来存储过去的状态 每一时刻的状态都由当前状态和上一时刻的状态共同决定 采用参数共享的思想，每一时刻的参数都是相同的，简化了模型中的参数，简化了运算。 可以与CNN进行对比，CNN同样采用了参数共享(权值共享)的思想 CNN中的参数共享是空间上的，其假设图像的底层特征与图像中的位置无关。(但是后来发现高级特征其实与位置同样有关系，需要用局部全连接层和全连接层混合来建模) RNN中的参数共享是时间上的，每一个时间步的参数都相同 ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:2","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"模型的数学表达 $$ H_t = \\sigma(W_HH_{t-1}+W{_X}X{_t}+b_h)$$ $$O_t = W_OH_t+b_q$$ 这里的$\\sigma$指的是非线性激活函数，在最原始的论文里用的是$Tanh$激活函数(后续改进中也有使用ReLU)，$W_H,W_X,W_O$分别是模型中的三个参数。 ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:3","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"梯度消失和梯度爆炸 RNN模型中一直令人诟病的两个问题是梯度消失和梯度爆炸问题，这两个问题的出现严重阻碍了模型的优化。下面通过随时间的反向传播(BPTT)进行公示的推导来演示梯度消失和梯度爆炸产生的原因。 符号定义 在进行推导之间，我们首先定义如下符号 符号 解释 $K$ 输入的向量维度 $T$ 输入元素的长度 $H$ 隐藏层单元数 $X= \\{ x_1,x_2,…,x_T \\} $ 输入元素的按时间表示表示 $x_t \\in \\mathbb{R}^{K \\times 1}$ $t$时刻RNN的输入 $h_t \\in \\mathbb{R}^{H \\times 1}$ $t$时刻，隐藏层的输出 $o_t \\in \\mathbb{R}^{H \\times 1}$ $t$时刻，模型的输出 $W_H \\in \\mathbb{R}^{H \\times H}$ 隐藏状态的权重参数 $W_X \\in \\mathbb{R}^{H \\times K}$ 输入的权重参数 $W_O \\in \\mathbb{R}^{K \\times H}$ 输出的权重参数 $L_t$ $t$时刻的损失函数的值 $L$ 所有时刻损失函数的值的总和 需要做说明的是上述符号定义并没有就具体问题进行分析，如果将其应用在不同问题上，可能有不同的符号定义方式，为了简单起见，本文仅以该设定为例，来讨论梯度消失和爆炸问题，其他问题均可采用该思想分析。 上述符号在RNN模型公式中的对应关系如下 $$\\begin{cases} h_t = tanh(W_H h_{t-1} + W_X x_{t}) \\\\ o_t = W_O h_t \\\\ L_t = f(o_t,y) \\\\ L = \\sum\\limits_{t=1}^T{L_t} \\end{cases}$$ 反向传播 在随时间反向传播方法中，我们需要求三个偏导数，$\\frac{\\partial L}{\\partial W_H},\\frac{\\partial L}{\\partial W_X},\\frac{\\partial L}{\\partial W_O}$。 1.计算$\\frac{\\partial L_t}{\\partial W_O}$ $$\\begin{aligned} \\frac{\\partial L_t}{\\partial W_O} = \\frac{\\partial L}{\\partial f(o_t,y)} \\cdot \\frac{\\partial f(o_t,y)}{\\partial o_t} \\cdot \\frac{\\partial o_t}{\\partial W_O} \\end{aligned}$$ 在此我们重点讨论$\\frac{\\partial o_t}{\\partial W_O}$的计算。从上述符号表和公式我们可知$o_t$是一个$H \\times 1$的列向量，$W_O$是一个$H \\times K$的矩阵 2.计算$\\frac{\\partial L_t}{\\partial W_H}$ $$ \\frac{\\partial L_t}{\\partial W_H} = \\frac{\\partial L}{\\partial f(o_t,y)} \\cdot \\frac{\\partial f(o_t,y)}{\\partial o_t} \\cdot \\frac{\\partial o_t}{\\partial h_t} \\cdot \\frac{\\partial h_t}{\\partial W_H} $$ 由于$h_t$是个复合函数，$h_{t-1}$同样包含$W_H$，因此根据链式求导法则和导数的乘法法则可以推出以下公式 $$ \\begin{aligned} \\frac{\\partial h_t}{\\partial W_H} \u0026= \\frac{\\partial h_t}{\\partial W_H} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial W_H} \\\\ \u0026= \\frac{\\partial h_t}{\\partial W_H} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial W_H} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\cdot \\frac{\\partial h_{t-1}}{\\partial h_{t-2}} \\cdot \\frac{\\partial h_{t-2}}{\\partial W_H} + ... \\end{aligned} $$ ","date":"2021-12-24","objectID":"/rnn%E5%92%8Clstm/:1:4","tags":["RNNs","深度学习","机器学习"],"title":"RNN和LSTM","uri":"/rnn%E5%92%8Clstm/"},{"categories":["RNNs","深度学习","机器学习"],"content":"IRNN模型1 ","date":"2021-12-23","objectID":"/irnn/:0:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"问题和动机 梯度消失和梯度爆炸的问题导致RNN模型难以学习到远距离依赖 过去的解决方法依赖于复杂的优化技术和网络架构 提出一种较为简单的方式进行优化 ","date":"2021-12-23","objectID":"/irnn/:1:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"过去的解决方法 用Hessian-Free来代替SGD2 3（Hessian-Free可以关注到曲率） 虽然效果上有改进，但并不常用，原因可能如下 比SGD基础上的方法更难实现 拟牛顿法需要更复杂的计算和内存开销 目标函数可能非凸 求得了更高精度的解可能不利于泛化 二阶法求得更高精度的解，但也需要更高精度的数据，深度学习数据本身存在很多随机误差，可能导致优化不稳定甚至失败 当使用了梯度裁剪4并关注了参数初始化5时，带动量的SGD与HF方法可以相当 最成功的改进LSTM 虽然能解决长距离依赖，但本文作者认为此结构并不是最优的结构 ","date":"2021-12-23","objectID":"/irnn/:2:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Idea 使用ReLU(Rectified Linear Units)激活函数 (trick)通过单位矩阵或者它的缩放版本来初始化RNN中的权重矩阵 作者认为以下工作与本文工作类似 本文作者认为其参数初始化方法与文献6中的方法相同，主要区别在于本文仅仅将单位矩阵用在初始化上，而且使用了ReLU激活函数。 scaled identity initialization同样在文献7中被提出，但没使用ReLU激活函数。 本文工作与文献8研究正交矩阵初始化同样类似。 ","date":"2021-12-23","objectID":"/irnn/:3:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"实验 实验中发现了下述炼丹技巧 为LSTM设置更高的forget gate bias能更好的解决长距离依赖关系 参数应用高斯随机初始化时使用文献9的值时效果更好。 实验中IRNN中非递归权重使用均值为0，标准差为0.001的高斯分布来初始化。分别展示了IRNN，标准LSTM，使用tanh激活的RNN和使用ReLU激活的RNN在4个实验上的结果。 ","date":"2021-12-23","objectID":"/irnn/:4:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"The Adding Problem实验 输入由两部分组成，第一部分是0-1范围内均匀分布的值，第二部分是掩码，最后的结果是经掩码后的两值得和Adding Problem实验 \" 输入由两部分组成，第一部分是0-1范围内均匀分布的值，第二部分是掩码，最后的结果是经掩码后的两值得和 通过在固定序列中随机抽取两个值，将其值求和作为输出，对此问题进行回归。使用MSE来评价效果，当一直预测值为1时，MSE为0.1767。实验结果如下所示 可以发现IRNN呈现出与LSTM相当的效果，但带有ReLU激活的RNN效果为何一直这么差?Adding Problem实验结果 \" 可以发现IRNN呈现出与LSTM相当的效果，但带有ReLU激活的RNN效果为何一直这么差? ","date":"2021-12-23","objectID":"/irnn/:4:1","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"MNIST Classification from a Sequence of Pixels实验 实验通过将图片的784个像素顺序输入网络中，在完全学习完784个像素后，再进行图片的分类。一次每个网络的循环步长为784。除了对原始图片进行预测的实验之外，还对图片的像素进行固定的随机重新排列后进行了第二次实验。 可以发现，IRNN性能最好，超过LSTM，但带有ReLU激活的RNN效果为何一直这么差?MNIST实验结果 \" 可以发现，IRNN性能最好，超过LSTM，但带有ReLU激活的RNN效果为何一直这么差? ","date":"2021-12-23","objectID":"/irnn/:4:2","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Language Modeling实验 4层IRNN与LSTM表现相当(同时LSTM的参数是单层IRNN的四倍)LanguageModel实验结果 \" 4层IRNN与LSTM表现相当(同时LSTM的参数是单层IRNN的四倍) ","date":"2021-12-23","objectID":"/irnn/:4:3","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"Speech Recognition实验 在该实验中作者发现使用单位矩阵初始化效果较差，本文猜测的原因如下 正常IRNN很难忘记过去的信息 难以专注当前的输入 我觉得等于没说，根据模型和结果猜原因 因此本文提出了补救办法，并认为其可以作为不需要长距离依赖时模型的补救办法 用一个小标量和单位矩阵的乘积来初始化(本文中使用的是0.01I) iRNN模型与LSTM相当，比标准RNN效果好Speech Recognition实验结果 \" iRNN模型与LSTM相当，比标准RNN效果好 ","date":"2021-12-23","objectID":"/irnn/:4:4","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"论文Insight 使用单位矩阵初始化循环参数(可能缓解梯度爆炸的问题) 使用ReLU激活(可以缓解梯度消失的问题) 二者共同作用可使得RNN模型性能得到改善 在不需要长期依赖时，可以使用小标量与I的乘积来进行初始化，类似于LSTM中的遗忘门机制 ","date":"2021-12-23","objectID":"/irnn/:5:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["RNNs","深度学习","机器学习"],"content":"思考中的问题 超参初始化问题，xariv，He，和本文中9提到的初始化的关系和效果 单位矩阵在模型中起到的作用(推导反向传播) 使用ReLU激活的RNN效果比tanh还差的原因 https://arxiv.org/abs/1504.00941 (A Simple Way to Initialize Recurrent Networks of Rectified Linear Units) ↩︎ Learning recurrent neural networks with Hessian-Free optimization. In ICML, 2011. ↩︎ Deep learning via Hessian-free optimization. In Proceedings of the 27th International Conference on Machine Learning, 2010. ↩︎ On the difficulty of training recurrent neural networks. ↩︎ On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning, 2013. ↩︎ Learning longer memory in recurrent neural networks ↩︎ Parsing with compositional vector grammars ↩︎ Exact solutions to the nonlinear dynamics of learning in deep linear neural networks ↩︎ Random Walk Initialization for Training Very Deep Feedforward Networks ↩︎ ","date":"2021-12-23","objectID":"/irnn/:6:0","tags":["RNNs","深度学习","机器学习","RNN优化"],"title":"IRNN","uri":"/irnn/"},{"categories":["运维"],"content":"运维问题记录 ","date":"2021-12-21","objectID":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:0:0","tags":["运维"],"title":"运维问题记录","uri":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["运维"],"content":"多网卡路由配置 可查询此链接linux网络命令查看linux网络命令的使用 使用策略路由 通过配置双路由表来配置不同网域使用不同路由表 具体方法 查看路由表存储情况 cat /etc/iproute2/rt_tables ​ 输出情况如下 # # reserved values # 255 local #本地路由表 254 main #主路由表，不加设定时我们增加的路由规则都设置于此 253 default #存放默认路由规则。注意增加默认规则时若没有指定路由表那还是存在于main表中 0 unspec # # local # #1 inr.ruhep route -n命令查看的是main路由表 查看对应路由表 ip route show table main ip route show table 254 修改rt_tables文件向指定路由表添加或删除规则使用ip route命令 ip route add 192.168.80.0/24 via 192.168.20.20 table 251 ip route add 192.168.80.0/24 via 192.168.30.20 table 252 ip route del 192.168.80.0/24 via 192.168.30.20 table 251 ip route del 192.168.80.0/24 via 192.168.30.20 table 252 快速删除某一特定路由或者路由表可使用ip route flush #清除192.168.0.0的路由信息 ip route flush 192.168.0.0 #清空main路由表 ip route flush table main 查看路由表策略 # ip rule show 或者 # ip rule ls 0: from all lookup local 32766: from all lookup main 32767: from all lookup default 创建策略( pref 越小越先匹配) #根据源地址决定路由表 ip rule add from 192.168.10.0/24 table 100 pref 10 ip rule add from 192.168.20.20 table 110 pref 100 #根据目的地址决定路由表 ip rule add to 192.168.30.0/24 table 120 ip rule add to 192.168.40.0/24 table 130 #根据网卡设备决定路由表 ip rule add dev eth0 table 140 ip rule add dev eth1 table 150 #此外还可以根据其他条件进行设置，例如tos等等 删除策略 #根据明细条目删除 ip rule del from 192.168.10.10 #根据优先级删除 ip rule del prio 32765 #根据表名称来删除 ip rule del table wangtong 或者使用netplan1配置，具体 修改/etc/netplan/*.yaml文件，示例如下 network:version:2renderer:networkdethernets:ens3:addresses:- 192.168.3.30/24dhcp4:noroutes:- to:192.168.3.0/24via:192.168.3.1table:101routing-policy:- from:192.168.3.0/24table:101priority:10nameservers:address:[192.168.3.1]gateway4:192.168.3.1ens5:addresses:- 192.168.5.24/24dhcp4:noroutes:- to:0.0.0.0/0via:192.168.5.1table:102- to:192.168.5.0/24via:192.168.5.1table:102routing-policy:- from:192.168.5.0/24table:102priority:10 当设置了gateway4之后将在默认的default表自动设置对应的默认路由 当render设置为NetWorkManager时,可能会提示不能设置没有默认路由的路由表,将render一行删除即可 ","date":"2021-12-21","objectID":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:1:0","tags":["运维"],"title":"运维问题记录","uri":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["运维"],"content":"设置开机启动自动运行的脚本 旧的版本中可以直接编辑 rc.local 添加开机启动脚本，而新版本这个功能默认是禁用的 Ubuntu20.04按下操作开启rc-local.service 给rc.local文件执行权限chmod +x或者chmod 755 vi /lib/systemd/system/rc-local.service添加如下代码 [Install] WantedBy=multi-user.target 启动服务systemctl enable rc-local Ubuntu18.04使用rc.local来对文件和服务命名,因此基本只需将20.04命令中的rc-loca改为rc.local即可 给rc.local文件执行权限chmod +x或者chmod 755 vi /lib/systemd/system/rc.local.service,添加如下代码 [Install] WantedBy=multi-user.target Alias=rc-local.service 启用服务systemctl enable rc.local.service ","date":"2021-12-21","objectID":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:2:0","tags":["运维"],"title":"运维问题记录","uri":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["运维"],"content":"ubuntu驱动内核更新导致nvidia驱动失效解决 sudo apt-get autoremove --purge nvidia-*删除nvidia相关包 sudo apt-get install linux-headers-$(uname -r)安装新内核的linux-headers,用于编译各种内核模块 sudo apt-get install nvidia-drivers-4**安装新的nvidia驱动 ","date":"2021-12-21","objectID":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:3:0","tags":["运维"],"title":"运维问题记录","uri":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["运维"],"content":"ubuntu更新内核 sudo apt-get install linux-image-version-generic安装Linux镜像 sudo apt-get install linux-image-extra-version-generic安装新内核的额外驱动 sudo apt-get install linux-headers-version-generic安装linux-headers https://netplan.io/examples/ ↩︎ ","date":"2021-12-21","objectID":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:4:0","tags":["运维"],"title":"运维问题记录","uri":"/%E8%BF%90%E7%BB%B4%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["命令"],"content":"关于git命令的介绍","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"git使用1 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:0:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"基本命令 创建仓库git init 把文件添加到仓库git add 全部添加 git add . 把文件提交到仓库git commit 可以多次git add后git commit 命令之后必须加参数 -m \"注释\"来提交 查看目前仓库情况git status git diff将本地文件与暂存区的文件进行比较 git log 查看文件历次commit提交的情况 commit后面的数字是提交的编号 HEAD表示当前版本,HEAD^表示上个版本，HEAD^^表示上上个版本 HEAD~100表示往上的100个版本 git reset 用法：git reset --hard HEAD^表示回退到前一个版本 发现最早的版本记录没了 此时可以通过git reset --hard 之前的版本号来恢复操作 git reflog 可以查看所有分支的所有操作记录（包括已经被删除的 commit 记录和 reset 的操作） 例如执行 git reset --hard HEAD~1，退回到上一个版本，用git log则是看不出来被删除的commitid，用git reflog则可以看到被删除的commitid，恢复到被删除的那个版本。 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:1:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"小结 创建git仓库使用git init命令 提交文件，对多个文件git add之后git commit -m \"\" 查看仓库情况，使用git status可以查看文件被修改，add了修改文件的情况，仓库没有发生变化三种状态 如果git status显示文件被修改，但还没被add，可以用git diff查看修改情况 查看版本提交历史情况使用git log HEAD表示当前版本,HEAD^表示上个版本，HEAD^^表示上上个版本HEAD~100表示往上的100个版本 版本回退使用git reset --hard commit_id/HEAD^^/HEAD~100 重返未来，使用git reflog查看提交历史命令 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:1:1","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"暂存区和master分支 git暂存区和master分支的概念 git是管理修改的，git add之后将文件添加到暂存区,如果没有将修改提交到暂存区，对文件的更改也不会上传到master分支上 如果对文件修改错误，但还没addgit checkout --readme.txt让文件恢复到最后一次git add或git commit的状态 如果错误的文件已经git add，使用git reset HEAD readme.txt可将暂存区的文件恢复至版本库文件的版本 当改乱了某文件内容，想直接丢弃工作区修改时，使用git checkout -- file 如果已经add了，文件在暂存区中，使用git reset HEAD \u003cfile\u003e 回到第一步，然后再按第一步操作 如果已经commit了，使用git log 然后使用git reset命令 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:2:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"删除文件 如果想要删除版本库里的文件，先删除本地文件,然后git rm删除文件，然后git commit git rm \u003cfile\u003e将文件从暂存区和工作区中删除 git rm --cached \u003cfile\u003e从暂存区移除，但仍然保留在当前工作目录中，仅是从跟踪清单中删除 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:3:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"链接远程库 git remote add origin ***关联远程库，***代表远程库的地址，可以为git或者https,git地址速度块 首次s git remote -v查看远程仓库 git push --force origin master或者git push -f origin master强制覆盖远程分支 git checkout -b a origin/a从远程a分支拉取分支信息到本地a分支 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:4:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"分支创建和基本使用 创建分支git branch dev 切换到分支git checkout dev 查看分支情况git branch 将别的分支合并到当前分支git merge 分支名 如果存在冲突，需要解决冲突 修改完冲突文件后使用git add .命令 再使用git commit命令，可以不带-m 参数添加注释 删除分支git branch -d 分支名 在操作中添加-r参数，代表对远程仓库进行分支操作 对一个分支的本地文件操作不会影响到另一分支的本地文件情况 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:5:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"Git进阶 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:6:0","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"分支管理 git rebase branch变基操作 将branch分支中的commit放到当前的commit之前，合并为同一分支 git commit --amend可以编辑当前的commit信息 git rebase -i HEAD~3 编辑前三个commit 通过squash可用于多个commit信息的合并 通过edit可用于历史commit信息的编辑 编辑完文件后仍需要多次运行git commit --amend来编辑多次历史提交信息 每次运行上述命令之后，需要git rebase --continue代表保存并进行下一条 --abort参数可放弃当前操作，例如git rebase --abort git push --set-upstream origin newbranch将本地分支newbranch与远程分支newbranch关联 origin/main和是远程分支在本地分支更新的克隆 git pull操作是git fetch和git merge origin/***两个命令的集合 git fetch命令使用远程分支更新本地origin/***分支 git merge origin/***将origin/***分支合并到当前***分支 ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:6:1","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"git status 中文文件名编码问题 在默认设置下，中文文件名在工作区状态输出，查看历史更改概要，以及在补丁文件中，文件名的中文不能正确地显示，而是显示为八进制的字符编码，示例如下： \"assets/\\346\\234\\272\\345\\231\\250\\345\\255\\246\\344\\271\\240\\347\\232\\204\\345\\271\\266\\350\\241\\214\\347\\255\\226\\347\\225\\245/\" \"content/posts/\\346\\234\\272\\345\\231\\250\\345\\255\\246\\344\\271\\240/\" 通过将Git配置变量 core.quotepath 设置为false，就可以解决中文文件名称在这些Git命令输出中的显示问题 git config --global core.quotepath false 将此改到github action文件中，同样可以解决hugo模板lastmod显示不正常的问题 https://www.liaoxuefeng.com/wiki/896043488029600 ↩︎ ","date":"2021-01-25","objectID":"/git%E4%BD%BF%E7%94%A8/:6:2","tags":["git","命令"],"title":"Git使用","uri":"/git%E4%BD%BF%E7%94%A8/"},{"categories":["命令"],"content":"关于linux的部分命令介绍","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"linux命令 小技巧： ctrl + shift + =放大终端字体 ctrl + -缩小终端字体 tab 若不存在歧义，则自动补全 若tab目标太多不能自动补全时，按两次tab可显示符合条件的目录 ctrl + c可退出 ctrl + a快速回到行首，ctrl + e快速回到行尾 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:0:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"1.linux基本命令 序号 命令 对应英文 作用 01 ls list 查看当前文件夹内容 02 pwd print work directory 查看当前所在文件夹 03 cd [目录名] change directory 切换文件夹 04 touch [文件名] touch 如果文件不存在，则新建文件 05 mkdir [目录名] make directory 创建目录 06 rm [文件名] remove 删除指定文件名 07 clear clear 清屏 08 cp 源文件 目标文件 copy 复制文件或目录 09 mv 源文件 目标文件 move 移动文件或者目录/文件或者目录重命名 10 cat [文件名] 查看文件所有内容 11 more [文件名] more 按页查看文件内容 12 grep [文本] [文件] 在文件中查找匹配的文本 注意事项 Linux下区分大小写 不加参数的rm只能删除文件，不能删除文件夹 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:1:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"2.Linux终端命令格式 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:2:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"2.1 终端命令格式 command [-options] [parameter] 说明： command:命令名 [-options]:选项 多选项时可单独列出 -a -b -c等，也可合在一起-abc [parameter]:传递的参数 []代表可选 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:2:1","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"2.2 查询命令帮助信息 2.2.1 –help command --help 说明： 显示command命令的帮助信息 2.2.2 man mam command 说明： 查询command命令的使用手册 man是manual的缩写，是linux提供的一个手册，包含了绝大多数命令的详细使用说明 使用man的快捷键 操作键 功能 空格 显示手册的下一屏 回车键 一次滚动手册页的一行 b 回滚一屏 f 前滚一屏 q 退出 /word 搜索word字符串 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:2:2","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.linux文件和目录命令 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.1命令总结 查看目录内容 ls tree以树状图列出文件目录结构 切换目录 cd 创建和删除操作 touch mkdir rm 拷贝和移动文件 cp move 查看文件内容 cat more grep ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:1","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.2 ls命令详细用法 1. Linux文件特点 以.开头的文件为隐藏文件，需要加参数-a才能显示 .代表当前目录 ..代表上级目录 2. ls命令常用选项 选项 含义 -a 显示指定目录下的所有文件和目录，包括隐藏 -l 以列表方式显示文件的详细信息 -h 配合-l 以人性化方式显示文件大小 3.ls通配符使用 通配符 含义 举例使用 * 代表任意个字符 ls 1*,ls 12*.txt,ls *.txt ？ 代表任意一个字符 ls 12?.txt,ls ?12.txt [] 创建字符组，表示可以匹配字符组中的任意一个 ls [123][123].txt [a-f] 匹配a到f所有字符中的一个 ls [1-3].txt,ls [a-f].txt ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:2","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.3 cd命令详细用法 1. cd命令常用用法 命令 cd 进入到用户家目录 cd ~ 进入到用户家目录 cd .. 返回到上级目录 cd - 在最近两次的工作目录中来回切换 2. 绝对路径和相对路径 绝对路径：输入路径开头是/或者~，表示从根目录或家目录开始的具体目录位置 相对路径：输入路径开头不是/或者~，表示相对当前目录的目录所在位置 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:3","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.4 创建和删除命令详细用法 1.touch命令 创建文件或修改文件时间 若文件不存在，则创建一个空白文件 若文件已经存在，则修改文件的末次修改日期 2.mkdir命令 创建一个新的目录 mkdir -p循环创建多个目录 例如**mkdir -p a/b/c/d** mkdir创建的目录路径下不能有与之同名的目录或文件 3.rm命令 rm删除直接从磁盘上删除文件，并不会放到垃圾桶，无法恢复 选项 含义 -f 强制删除，忽略不存在的文件，且不提示 -r 递归删除一个目录下所有内容，删除文件夹时使用 rm命令也可以使用通配符 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:4","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.5 拷贝和移动命令 1. tree命令 tree命令可以以树状图列出文件目录结构 不加参数显示当前目录的树形结构 tree [目录]显示指定目录的结构 参数 含义 -d 只显示目录 2.cp命令 cp命令的功能是将给出的文件或目录复制到另一个文件或者目录中 cp 源路径/源文件名 目标路径/目标名 同时可以给目标重命名 cp并不能直接复制目录 cp ~/documents/123.txt ./readme.txt 选项 含义 -i 提示覆盖信息 -r 复制目录 3. mv命令 mv命令可以移动文件和重命名 例：mv ./desktop/123.txt ./documents/(234.txt) ps:123.txt会消失,()代表可选 mv 123(.txt) 2(.txt)可以重命名 注意事项： 假设已经存在了234.txt，则会用123.txt覆盖234.txt 参数 含义 -i 覆盖文件前提示 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:5","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.6 查看和查找文件内容 1. cat 命令 cat 123.txt可以显示123.txt的所有内容 选项 含义 -b 按非空行编号并输出 -n 按所有行编号并输出 2.more命令 more 1.txt可以实现一屏的内容 操作键 功能 空格 显示手册的下一屏 回车键 一次滚动手册页的一行 b 回滚一屏 f 前滚一屏 q 退出 3.grep命令 grep 文本内容 文件 grep \"hello world\" 1.txt当匹配的字符串含空格，则用“”框起来 选项 含义 -n 显示匹配行及行号 -v 显示不包括匹配文本的所有行 -i 忽略大小写 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:6","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"3.7 重定向和管道 echo命令会在终端中显示参数指定的文字，通常和重定向联合使用 1.重定向 \u003e和» 可以将命令的执行结果重定向到一个文件中 将本应输出在终端的结果输出或追加到其他文件上 \u003e输出到文件中，覆盖原文件内容 \u003e\u003e输出到文件中，追加到原文件最后 2. 管道| ls -lh | more类似这样的用法 将第一个命令的输出作为第二个命令的输入 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:3:7","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.远程管理命令 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:0","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.1命令总结 序号 命令 英文 作用 01 shutdown 选项 时间 shutdown 关机/重新启动 02 ifconfig configure a network interface 查看/配置计算机当前的网卡配置 03 ping IP地址 ping 检测到目标ip地址的连接是否正常 04 ssh [-p port] user@remote ssh客户端命令 连接远程ssh服务器 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:1","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.2 关机和重启命令 shutdown可以安全关闭或者重新启动系统 选项 含义 -r 重新启动 提示： 不指定选项和参数时，默认1分钟后关闭电脑 shutdown shutdown now shutdown -r now shutdown 20:25 //20：25时关机 shutdown +10 //十分钟后关机 shutdown -c//取消关机动作 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:2","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.3 查看和配置网卡信息 4.3.1 网卡和IP地址 网卡是一个专门负责网络通讯的硬件设备 ip地址是设置在网卡上的地址信息，不能重复 电脑是电话，网卡是SIM卡，IP地址是电话号 4.3.2 ifconfig命令 ifconfig可以查看/配置计算机当前的网卡配置信息 # 查看网卡配置信息 $ ifconfig # 关闭网卡 $ ifconfig 网卡 down # 打开网卡 $ ifconfig 网卡 up # 查看网卡对应的ip地址 $ ifconfig| grep inet 127.0.0.1被称为本地回环/环回地址，一般用来测试本机网卡是否正常 4.3.3 ping命令 # 检测到目标主机是否连接正常 $ ping ip地址 # 检查本地网卡是否工作正常 $ ping 127.0.0.1 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:3","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"},{"categories":["命令"],"content":"4.4 远程登录和复制文件 4.4.1 ssh基础 通过ssh客户端 登录管理 ssh服务器 在linux/macOS上 ssh服务器和ssh客户端都是自动安装的，只有windows需要自己安装 ssh客户端是一种使用Secure Shell(SSH)协议连接到远程计算机的软件程序 SSH数据传输是压缩且加密的 1）域名和端口号 1.域名 www.baidu.com类似如此的名字，是IP地址的别名，方便用户记忆 2.端口号 IP地址：通过IP地址找到计算机 端口号：通过此找到计算机上的应用程序 常见服务器端口列表 序号 服务 端口号 01 SSH服务器 22 02 Web服务器 80 03 HTTPS 443 04 FTP服务器 21 如果是默认端口号，在连接时可以省略 否则 IP地址：端口号 2）ssh客户端的简单使用 ssh [-p port] user@remote user是远程机器上的用户名，不指定的话默认为当前用户 remote是远程机器的地址，可以是ip/域名或者是别名 port是SSH Server监听的端口，如果不指定，就为默认值22 提示： 使用exit退出当前用户的登录 4.4.2 scp命令 scp命令，是一个在linux下进行远程拷贝文件的命令 scp地址格式与ssh基本相同，但在指定端口时，使用-P而不是-p # 把本地目录下的01.py复制到 远程 家目录下的Desktop/01.py # `:`后面的路径不是绝对路径，则以用户的家目录作为参照路径 scp -p port 01.py user@remote:Desktop/01.py # 把远程 家目录下的Desktop/01.py 辅助到 本地当前目录下的01.py scp -p port user@remote:Desktop/01.py 01.py #加上r可以传送文件夹 选项 含义 -P 加端口 -r 拷贝目录 ","date":"2021-01-22","objectID":"/linux%E5%91%BD%E4%BB%A4/:4:4","tags":["linux","命令"],"title":"Linux命令","uri":"/linux%E5%91%BD%E4%BB%A4/"}]