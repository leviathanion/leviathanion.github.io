# SRU

# SRU
## 问题和动机
* 模型的性能依赖于模型的参数和计算量，但RNNs的并行计算能力严重拖累了模型的训练
* RNNs的可扩展能力主要受限于RNNs模型的计算序列依赖性，最近的模型为了可扩展性广泛使用的是CNN和注意力机制
* 因此作者提出了一种平衡了循环依赖和独立性的新模型SRU
    * SRU的状态计算与时间相关，但每个状态都是独立的，可以实现CUDA级优化。
    * 摒弃了卷积的使用，具有更多的循环，保留了建模能力，减少了计算复杂度和超参数。
    * 使用highway连接和精细的参数初始化方案，改进了模型的训练
## 过去的解决方法
* QRNN将字符级卷积合并到RNN模型中，加速模型计算
* IRNN使用单位对角矩阵初始化RNN的隐藏层的参数，防止梯度消失
* [^2]通过将矩阵进行分解从而减少参数量(本质上就是将参数矩阵`(m*n)`直接写为`(m*r)和(r*n)`使得，`m*r+r*n小于m*n`)

